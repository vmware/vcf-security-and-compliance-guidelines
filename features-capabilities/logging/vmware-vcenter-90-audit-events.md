# VMware vCenter 9.0 Event Reference

This reference provides information about the events generated by VMware vCenter in response to various system activities. This document also includes the parameters associated with each event. These events can be viewed in the vSphere Client, accessed via the vSphere API, and forwarded to external log collectors.

## Disclaimer

This document is intended to provide general guidance for organizations that are considering Broadcom solutions. The information contained in this document is for educational and informational purposes only. This document is not intended to provide advice and is provided "AS IS." Broadcom makes no claims, promises, or guarantees about the accuracy, completeness, or adequacy of the information contained herein. Organizations should engage appropriate legal, business, technical, and audit expertise within their specific organization for review of requirements and effectiveness of implementations.

## VMware ESXi versus VMware ESX

With the release of VMware Cloud Foundation 9.0 the name of the VMware Hypervisor was changed from ESXi back to ESX. Documents such as this, which use information that span a range of release versions, may use the names ESXi and ESX interchangeably, or refer to the hypervisor solely as ESX for simplicity. Unless you are running VMware vSphere 4.1, please consider both ESXi and ESX to be the same, and use the product version to determine applicability to your environment.

# Events

**Total Events:** 685

| **Event Name** | **Severity** | **Description** | **Full Format** | **Parameters** |
|----------------|--------------|-----------------|-----------------|----------------|
| ad.event.ImportCertEvent | Info | Import certificate success | `Import certificate succeeded.` |  |
| ad.event.ImportCertFailedEvent | Error | Import certificate failure | `Import certificate failed.` |  |
| ad.event.JoinDomainEvent | Info | Join domain success | `Join domain succeeded.` |  |
| ad.event.JoinDomainFailedEvent | Error | Join domain failure | `Join domain failed.` |  |
| ad.event.LeaveDomainEvent | Info | Leave domain success | `Leave domain succeeded.` |  |
| ad.event.LeaveDomainFailedEvent | Error | Leave domain failure | `Leave domain failed.` |  |
| com.vmware.vc.HA.CreateConfigVvolFailedEvent | Error | vSphere HA failed to create a configuration vVol for this datastore and so will not be able to protect virtual machines on the datastore until the problem is resolved. Error: {fault} | `vSphere HA failed to create a configuration vVol for this datastore and so will not be able to protect virtual machines on the datastore until the problem is resolved. Error: {fault}` | <ul><li>{fault}: Fault message</li></ul> |
| com.vmware.vc.HA.CreateConfigVvolSucceededEvent | Info | vSphere HA successfully created a configuration vVol after the previous failure | `vSphere HA successfully created a configuration vVol after the previous failure` |  |
| com.vmware.vc.HA.DasHostCompleteDatastoreFailureEvent | Error | Host complete datastore failure | `All shared datastores failed on the host {hostName} in cluster {computeResource.name} in {datacenter.name}` | <ul><li>{hostName}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.HA.DasHostCompleteNetworkFailureEvent | Error | Host complete network failure | `All VM networks failed on the host {hostName} in cluster {computeResource.name} in {datacenter.name}` | <ul><li>{hostName}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.VmCloneFailedInvalidDestinationEvent | Error | Cannot complete virtual machine clone. | `Cannot clone {vm.name} as {destVmName} to invalid or non-existent destination with ID {invalidMoRef}: {fault}` | <ul><li>{vm.name}: VM name</li><li>{destVmName}: Destination VM name</li><li>{invalidMoRef}: Invalid object reference</li><li>{fault}: Fault message</li></ul> |
| com.vmware.vc.VmCloneToResourcePoolFailedEvent | Error | Cannot complete virtual machine clone. | `Cannot clone {vm.name} as {destVmName} to resource pool {destResourcePool}: {fault}` | <ul><li>{vm.name}: VM name</li><li>{destVmName}: Destination VM name</li><li>{destResourcePool}: Destination resource pool</li><li>{fault}: Fault message</li></ul> |
| com.vmware.vc.VmDiskConsolidatedEvent | Info | Virtual machine disks consolidation succeeded. | `Virtual machine {vm.name} disks consolidated successfully on {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.VmDiskConsolidationNeeded | Warning | Virtual machine disks consolidation needed. | `Virtual machine {vm.name} disks consolidation is needed on {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.VmDiskConsolidationNoLongerNeeded | Info | Virtual machine disks consolidation no longer needed. | `Virtual machine {vm.name} disks consolidation is no longer needed on {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.VmDiskFailedToConsolidateEvent | Warning | Virtual machine disks consolidation failed. | `Virtual machine {vm.name} disks consolidation failed on {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.datastore.UpdateVmFilesFailedEvent | Error | Failed to update VM files | `Failed to update VM files on datastore {ds.name} using host {hostName}` | <ul><li>{ds.name}: Datastore name</li><li>{hostName}: Host name</li></ul> |
| com.vmware.vc.datastore.UpdatedVmFilesEvent | Info | Updated VM files | `Updated VM files on datastore {ds.name} using host {hostName}` | <ul><li>{ds.name}: Datastore name</li><li>{hostName}: Host name</li></ul> |
| com.vmware.vc.datastore.UpdatingVmFilesEvent | Info | Updating VM Files | `Updating VM files on datastore {ds.name} using host {hostName}` | <ul><li>{ds.name}: Datastore name</li><li>{hostName}: Host name</li></ul> |
| com.vmware.vc.ft.VmAffectedByDasDisabledEvent | Warning | Fault Tolerance VM restart disabled | `vSphere HA has been disabled in cluster {computeResource.name} of datacenter {datacenter.name}. vSphere HA will not restart VM {vm.name} or its Secondary VM after a failure.` | <ul><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{vm.name}: VM name</li></ul> |
| com.vmware.vc.guestOperations.GuestOperation | Info | Guest operation | `Guest operation {operationName.@enum.com.vmware.vc.guestOp} performed on Virtual machine {vm.name}.` | <ul><li>{operationName}: Operation name</li><li>{vm.name}: VM name</li></ul> |
| com.vmware.vc.guestOperations.GuestOperationAuthFailure | Warning | Guest operation authentication failure | `Guest operation authentication failed for operation {operationName.@enum.com.vmware.vc.guestOp} on Virtual machine {vm.name}.` | <ul><li>{operationName}: Operation name</li><li>{vm.name}: VM name</li></ul> |
| com.vmware.vc.host.PartialMaintenanceModeStatusChanged | Info | Partial maintenance mode status has changed. | `Host status for '{id.@enum.host.PartialMaintenanceModeId}' is now '{status.@enum.host.PartialMaintenanceModeStatus} partial maintenance mode'.` | <ul><li>{id}: ID</li><li>{status}: Status</li></ul> |
| com.vmware.vc.host.clear.vFlashResource.inaccessible | Info | Host's virtual flash resource is accessible. | `Host's virtual flash resource is restored to be accessible.` |  |
| com.vmware.vc.host.clear.vFlashResource.reachthreshold | Info | Host's virtual flash resource usage dropped below the threshold. | `Host's virtual flash resource usage dropped below {1}%.` | <ul><li>{1}: Percentage</li></ul> |
| com.vmware.vc.host.problem.DeprecatedVMFSVolumeFound | Warning | Deprecated VMFS volume(s) found on the host. Please consider upgrading volume(s) to the latest version. | `Deprecated VMFS volume(s) found on the host. Please consider upgrading volume(s) to the latest version.` |  |
| com.vmware.vc.host.problem.DeprecatedVMFSVolumeFoundAfterVMFS3EOL | Warning | Deprecated VMFS (ver 3) volumes found. Upgrading such volumes to VMFS (ver 5) is mandatory for continued availability on vSphere 6.7 hosts | `Deprecated VMFS (ver 3) volumes found. Upgrading such volumes to VMFS (ver 5) is mandatory for continued availability on vSphere 6.7 hosts` |  |
| com.vmware.vc.host.problem.VStorageObjectInfraCatalogUnhealthy | Warning | Improved virtual disk infrastructure's catalog management turned unhealthy | `Improved virtual disk infrastructure's catalog management turned unhealthy` |  |
| com.vmware.vc.host.problem.VStorageObjectInfraNamespacePolicyEmptyEvent | Warning | Improved virtual disk infrastructure namespaces are created with empty storage policy. Please consider updating infrastructure namespace storage policy to avoid potential data loss. | `Improved virtual disk infrastructure namespaces are created with empty storage policy. Please consider updating infrastructure namespace storage policy to avoid potential data loss.` |  |
| com.vmware.vc.host.problem.vFlashResource.inaccessible | Warning | Host's virtual flash resource is inaccessible. | `Host's virtual flash resource is inaccessible.` |  |
| com.vmware.vc.host.problem.vFlashResource.reachthreshold | Warning | Host's virtual flash resource usage exceeds the threshold. | `Host's virtual flash resource usage is more than {1}%.` | <ul><li>{1}: Percentage</li></ul> |
| com.vmware.vc.host.vFlash.VFlashResourceConfiguredEvent | Info | Virtual flash resource is configured on the host | `Virtual flash resource is configured on the host` |  |
| com.vmware.vc.host.vFlash.VFlashResourceRemovedEvent | Info | Virtual flash resource is removed from the host | `Virtual flash resource is removed from the host` |  |
| com.vmware.vc.host.vFlash.defaultModuleChangedEvent | Info | Default virtual flash module is changed to {vFlashModule} on the host | `Any new virtual Flash Read Cache configuration request will use {vFlashModule} as default virtual flash module. All existing virtual Flash Read Cache configurations remain unchanged.` | <ul><li>{vFlashModule}: vFlash module</li></ul> |
| com.vmware.vc.host.vFlash.modulesLoadedEvent | Info | Virtual flash modules are loaded or reloaded on the host | `Virtual flash modules are loaded or reloaded on the host` |  |
| com.vmware.vc.npt.VmAdapterEnteredPassthroughEvent | Info | Virtual NIC entered passthrough mode | `Network passthrough is active on adapter {deviceLabel} of virtual machine {vm.name} on host {host.name} in {datacenter.name}` | <ul><li>{deviceLabel}: Device label</li><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.npt.VmAdapterExitedPassthroughEvent | Info | Virtual NIC exited passthrough mode | `Network passthrough is inactive on adapter {deviceLabel} of virtual machine {vm.name} on host {host.name} in {datacenter.name}` | <ul><li>{deviceLabel}: Device label</li><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vcp.FtDisabledVmTreatAsNonFtEvent | Info | FT Disabled VM protected as non-FT VM | `HA VM Component Protection protects virtual machine {vm.name} on {host.name} in cluster {computeResource.name} in datacenter {datacenter.name} as non-FT virtual machine because the FT state is disabled` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vcp.FtFailoverEvent | Info | Failover FT VM due to component failure | `FT Primary VM {vm.name} on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name} is going to fail over to Secondary VM due to component failure ` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vcp.FtFailoverFailedEvent | Error | FT VM failover failed | `FT virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name} failed to failover to secondary` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vcp.FtSecondaryRestartEvent | Info | Restarting FT secondary due to component failure | `HA VM Component Protection is restarting FT secondary virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name} due to component failure` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vcp.FtSecondaryRestartFailedEvent | Error | FT secondary VM restart failed | `FT Secondary VM {vm.name} on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name} failed to restart` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vcp.NeedSecondaryFtVmTreatAsNonFtEvent | Info | Need secondary VM protected as non-FT VM | `HA VM Component Protection protects virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name} as non-FT virtual machine because it has been in the needSecondary state too long` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vcp.TestEndEvent | Info | VM Component Protection test ends | `VM Component Protection test ends on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name}` | <ul><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vcp.TestStartEvent | Info | VM Component Protection test starts | `VM Component Protection test starts on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name}` | <ul><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vcp.VcpNoActionEvent | Info | No action on VM | `HA VM Component Protection did not take action on virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name} due to the feature configuration setting` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vcp.VmDatastoreFailedEvent | Error | Virtual machine lost datastore access | `Virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name} lost access to {datastore}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{datastore}: Datastore</li></ul> |
| com.vmware.vc.vcp.VmNetworkFailedEvent | Error | Virtual machine lost VM network accessibility | `Virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name} lost access to {network}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{network}: Network</li></ul> |
| com.vmware.vc.vcp.VmPowerOffHangEvent | Error | VM power off hang | `HA VM Component Protection could not power off virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name} successfully after trying {numTimes} times and will keep trying` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{numTimes}: num Times</li></ul> |
| com.vmware.vc.vcp.VmRestartEvent | Info | Restarting VM due to component failure | `HA VM Component Protection is restarting virtual machine {vm.name} due to component failure on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vcp.VmRestartFailedEvent | Error | Virtual machine affected by component failure failed to restart | `Virtual machine {vm.name} affected by component failure on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name} failed to restart` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vcp.VmWaitForCandidateHostEvent | Error | No candidate host to restart | `HA VM Component Protection could not find a destination host for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in datacenter {datacenter.name} after waiting {numSecWait} seconds and will keep trying` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{numSecWait}: num Sec Wait</li></ul> |
| com.vmware.vc.vm.VmStateFailedToRevertToSnapshot | Error | Failed to revert the virtual machine state to a snapshot | `Failed to revert the execution state of the virtual machine {vm.name} on host {host.name}, in compute resource {computeResource.name} to snapshot {snapshotName}, with ID {snapshotId}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{snapshotName}: Name</li><li>{snapshotId}: snapshot Id</li></ul> |
| com.vmware.vc.vm.VmStateRevertedToSnapshot | Info | The virtual machine state has been reverted to a snapshot | `The execution state of the virtual machine {vm.name} on host {host.name}, in compute resource {computeResource.name} has been reverted to the state of snapshot {snapshotName}, with ID {snapshotId}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{snapshotName}: Name</li><li>{snapshotId}: snapshot Id</li></ul> |
| com.vmware.vc.vmam.AppMonitoringNotSupported | Warning | Application Monitoring Is Not Supported | `Application monitoring is not supported on {host.name} in cluster {computeResource.name} in {datacenter.name}` | <ul><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vmam.VmAppHealthMonitoringStateChangedEvent | Warning | vSphere HA detected application heartbeat status change | `vSphere HA detected that the application heartbeat status changed to {status.@enum.VirtualMachine.AppHeartbeatStatusType} for {vm.name} on {host.name} in cluster {computeResource.name} in {datacenter.name}` | <ul><li>{status}: Status</li><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vmam.VmAppHealthStateChangedEvent | Warning | vSphere HA detected application state change | `vSphere HA detected that the application state changed to {state.@enum.vm.GuestInfo.AppStateType} for {vm.name} on {host.name} in cluster {computeResource.name} in {datacenter.name}` | <ul><li>{state}: state</li><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vc.vmam.VmDasAppHeartbeatFailedEvent | Warning | vSphere HA detected application heartbeat failure | `vSphere HA detected application heartbeat failure for {vm.name} on {host.name} in cluster {computeResource.name} in {datacenter.name}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| com.vmware.vim.dpu.down | Info | A Data Processing Unit is down. | `The Data Processing Unit with id '{dpuId}' is down.` | <ul><li>{dpuId}: dpu Id</li></ul> |
| com.vmware.vim.dpu.removed | Info | A Data Processing Unit has been removed from the system. | `The Data Processing Unit with id '{dpuId}' has been removed from the system.` | <ul><li>{dpuId}: dpu Id</li></ul> |
| com.vmware.vim.dpu.state.changed | Info | The management state for a Data Processing Unit has changed. | `The management state for the Data Processing Unit with id '{dpuId}' has changed to '{state}'.` | <ul><li>{dpuId}: dpu Id</li><li>{state}: state</li></ul> |
| com.vmware.vim.dpuFailover.end | Info | The dpu failover ended on host. | `DPU failover from {fromDpu} to {toDpu} on vds {vds} has ended.` | <ul><li>{fromDpu}: from Dpu</li><li>{toDpu}: to Dpu</li><li>{vds}: vds</li></ul> |
| com.vmware.vim.dpuFailover.start | Info | The dpu failover started on host. | `DPU failover from {fromDpu} to {toDpu} on vds {vds} has been started.` | <ul><li>{fromDpu}: from Dpu</li><li>{toDpu}: to Dpu</li><li>{vds}: vds</li></ul> |
| com.vmware.vim.utf8filter.badvalue | Warning | Invalid UTF-8 string encountered. | `Invalid UTF-8 string encountered.` |  |
| com.vmware.vim.vm.DisksNotLoaded | Warning | Some of the disks of the virtual machine failed to load. The information present for them in the virtual machine configuration may be incomplete | `Some of the disks of the virtual machine {vm.name} on host {host.name} failed to load. The information present for them in the virtual machine configuration may be incomplete` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li></ul> |
| com.vmware.vim.vm.SnapshotNotAllowed | Warning | Snapshot operations are not allowed due to some of the snapshot related objects failed to load. | `Snapshot operations are not allowed on virtual machine {vm.name} due to some of the snapshot related objects failed to load.` | <ul><li>{vm.name}: VM name</li></ul> |
| com.vmware.vim.vm.reboot.powerOff | Info | Virtual machine reboot converted to power off because the rebootPowerOff option is enabled | `Reboot converted to power off on virtual machine {vm.name} on {host.name} because the rebootPowerOff option is enabled.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li></ul> |
| esx.audit.account.locked | Warning | Remote access for an ESXi local user account has been locked temporarilly due to multiple failed login attempts. | `Remote access for ESXi local user account '{1}' has been locked for {2} seconds after {3} failed login attempts.` | <ul><li>{1}: Value</li><li>{2}: Seconds</li><li>{3}: Failure count</li></ul> |
| esx.audit.account.loginfailures | Warning | Multiple remote login failures detected for an ESXi local user account. | `Multiple remote login failures detected for ESXi local user account '{1}'.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.agent.hostd.started | Info | VMware Host Agent started | `VMware Host Agent started on host {host.name}.` | <ul><li>{host.name}: Host name</li></ul> |
| esx.audit.agent.hostd.stopped | Info | VMware Host Agent stopped | `VMware Host Agent stopped on host {host.name}.` | <ul><li>{host.name}: Host name</li></ul> |
| esx.audit.dcui.defaults.factoryrestore | Warning | Restoring factory defaults through DCUI. | `The host has been restored to default factory settings. Please consult ESXi Embedded and vCenter Server Setup Guide or follow the Ask VMware link for more information.` |  |
| esx.audit.dcui.disabled | Info | The DCUI has been disabled. | `The DCUI has been disabled.` |  |
| esx.audit.dcui.enabled | Info | The DCUI has been enabled. | `The DCUI has been enabled.` |  |
| esx.audit.dcui.host.reboot | Warning | Rebooting host through DCUI. | `The host is being rebooted through the Direct Console User Interface (DCUI).` |  |
| esx.audit.dcui.host.shutdown | Warning | Shutting down host through DCUI. | `The host is being shut down through the Direct Console User Interface (DCUI).` |  |
| esx.audit.dcui.hostagents.restart | Info | Restarting host agents through DCUI. | `The management agents on the host are being restarted. Please consult ESXi Embedded and vCenter Server Setup Guide or follow the Ask VMware link for more information.` |  |
| esx.audit.dcui.login.failed | Error | Login authentication on DCUI failed | `Authentication of user {1} has failed. Please consult ESXi Embedded and vCenter Server Setup Guide or follow the Ask VMware link for more information.` | <ul><li>{1}: User</li></ul> |
| esx.audit.dcui.login.passwd.changed | Info | DCUI login password changed. | `Login password for user {1} has been changed. Please consult ESXi Embedded and vCenter Server Setup Guide or follow the Ask VMware link for more information.` | <ul><li>{1}: User</li></ul> |
| esx.audit.dcui.network.factoryrestore | Warning | Factory network settings restored through DCUI. | `The host has been restored to factory network settings. Please consult ESXi Embedded and vCenter Server Setup Guide or follow the Ask VMware link for more information.` |  |
| esx.audit.dcui.network.restart | Info | Restarting network through DCUI. | `A management interface {1} has been restarted. Please consult ESXi Embedded and vCenter Server Setup Guide or follow the Ask VMware link for more information.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.entropy.available.low | Warning | Host is configured with external entropy source. Host is running low on entropy bits in its memory cache. Please refer to KB 89074 for more details. | `Host is configured with external entropy source. Host is running low on entropy bits in its memory cache. Please refer to KB 89074 for more details.` |  |
| esx.audit.entropy.external.source.disconnected | Warning | Host is configured with external entropy source. The external entropy source is disconnected. Please refer to KB 89074 for more details. | `Host is configured with external entropy source. The external entropy source is disconnected. Please refer to KB 89074 for more details.` |  |
| esx.audit.esxcli.host.poweroff.reason | Warning | Powering off host through esxcli | `The host is being powered off through esxcli. Reason for powering off: {1}, User: {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.esxcli.host.reboot.reason | Warning | Rebooting host through esxcli | `The host is being rebooted through esxcli. Reason for reboot: {1}, User: {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.esxcli.host.restart.reason | Warning | Rebooting host through esxcli | `The host is being rebooted through esxcli. Reason for reboot: {1}, User: {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.esximage.hostacceptance.changed | Info | Host acceptance level changed | `Host acceptance level changed from {1} to {2}` | <ul><li>{1}: Timestamp (start)</li><li>{2}: Timestamp (end)</li></ul> |
| esx.audit.esximage.install.nobypasssigcheck | Warning | UEFI Secure Boot enabled: Cannot skip signature checks. | `UEFI Secure Boot enabled: Cannot skip signature checks. Installing unsigned VIBs will prevent the system from booting. So the vib signature check will be enforced.` |  |
| esx.audit.esximage.install.nosigcheck | Warning | Attempting to install an image profile bypassing signing and acceptance level verification. | `Attempting to install an image profile bypassing signing and acceptance level verification. This may pose a large security risk.` |  |
| esx.audit.esximage.install.novalidation | Warning | Attempting to install an image profile with validation disabled. | `Attempting to install an image profile with validation disabled.  This may result in an image with unsatisfied dependencies, file or package conflicts, and potential security violations.` |  |
| esx.audit.esximage.install.securityalert | Warning | SECURITY ALERT: Installing image profile. | `SECURITY ALERT: Installing image profile '{1}' with {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.esximage.profile.install.successful | Info | Successfully installed image profile. | `Successfully installed image profile '{1}'. Installed {2} VIB(s), removed {3} VIB(s). Please use 'esxcli software profile get' or see log for more detail about the transaction.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.esximage.profile.update.successful | Info | Successfully updated host to new image profile. | `Successfully updated host to image profile '{1}'. Installed {2} VIB(s), removed {3} VIB(s). Please use 'esxcli software profile get' or see log for more detail about the transaction.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.esximage.software.apply.succeeded | Info | Successfully changed software on host. | `Successfully installed {1} component(s) and removed {2} component(s) on host. To see more details about the transaction, run 'esxcli software profile get'.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.esximage.vib.install.successful | Info | Successfully installed VIBs. | `Successfully installed {1} VIB(s), removed {2} VIB(s). Please use 'esxcli software profile get' or see log for more detail about the transaction.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.esximage.vib.remove.successful | Info | Successfully removed VIBs | `Successfully removed {1} VIB(s). Please use 'esxcli software profile get' or see log for more detail about the transaction.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.esxtokend.dputrust.failed | Error | DPU trust validation failed | `DPU: {1} trust validation failed` | <ul><li>{1}: Value</li></ul> |
| esx.audit.esxtokend.dputrust.removed | Warning | DPU was removed | `DPU:{1} was removed.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.esxtokend.dputrust.succeeded | Info | DPU trust validation succeeded | `DPU: {1} trust validation succeeded.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.hardware.cxl.device.issue.discovered | Info | This host has CXL devices, which are not supported and can have unintended functionality or performance consequences. | `This host has CXL devices, which are not supported and can have unintended functionality or performance consequences.` |  |
| esx.audit.hardware.nvd.health.alarms.es.lifetime.warning | Warning | NVDIMM: Energy Source Lifetime Warning tripped. | `NVDIMM (handle {1}, idString {2}): Energy Source Lifetime ({3}) Warning tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.hardware.nvd.health.alarms.es.temperature.warning | Warning | NVDIMM: Energy Source Temperature Warning tripped. | `NVDIMM (handle {1}, idString {2}): Energy Source Temperature ({3} C) Warning tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.hardware.nvd.health.alarms.lifetime.warning | Warning | NVDIMM: Lifetime Warning tripped. | `NVDIMM (handle {1}, idString {2}): Lifetime ({3}) Warning tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.hardware.nvd.health.alarms.spareblocks | Warning | NVDIMM (handle {1}, idString {2}): SpareBlocksPct ({3}) has reached the pre-programmed threshold limit. | `NVDIMM (handle {1}, idString {2}): SpareBlocksPct ({3}) has reached the pre-programmed threshold limit.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.hardware.nvd.health.alarms.temperature | Warning | NVDIMM (handle {1}, idString {2}): Temperature ({3} C) has reached the pre-programmed threshold limit. | `NVDIMM (handle {1}, idString {2}): Temperature ({3} C) has reached the pre-programmed threshold limit.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.hardware.nvd.health.life.pctused | Warning | NVDIMM (handle {1}, idString {2}): Life Percentage Used ({3}) has reached the threshold limit ({4}). | `NVDIMM (handle {1}, idString {2}): Life Percentage Used ({3}) has reached the threshold limit ({4}).` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.audit.hardware.nvd.health.module.ce | Info | NVDIMM Count of DRAM correctable ECC errors above threshold. | `NVDIMM (handle {1}, idString {2}): Count of DRAM correctable ECC errors above threshold.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.hardware.nvd.health.vmw.alarms.es.lifetime.warning | Warning | NVDIMM: Energy Source Lifetime Warning tripped. | `NVDIMM (handle {1}, idString {2}): Energy Source Lifetime Warning tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.hardware.nvd.health.vmw.alarms.es.temperature.warning | Warning | NVDIMM: Energy Source Temperature Warning tripped. | `NVDIMM (handle {1}, idString {2}): Energy Source Temperature Warning tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.hardware.nvd.health.vmw.alarms.module.lifetime.warning | Warning | NVDIMM: Module Lifetime Warning tripped. | `NVDIMM (handle {1}, idString {2}): Module Lifetime Warning tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.hardware.nvd.health.vmw.alarms.module.temperature.warning | Warning | NVDIMM: Module Temperature Warning tripped. | `NVDIMM (handle {1}, idString {2}): Module Temperature Warning tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.hardware.nvd.health.vmw.statusflags.maintNeeded | Warning | NVDIMM: Maintenance needed. | `NVDIMM (handle {1}, idString {2}): Maintenance needed.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.hcm.event.disk.insertion | Info | A physical disk has been inserted. | `A physical disk has been inserted ({1}).` | <ul><li>{1}: Value</li></ul> |
| esx.audit.hcm.event.disk.removal | Info | A physical disk has been removed. | `A physical disk has been removed ({1}).` | <ul><li>{1}: Value</li></ul> |
| esx.audit.host.boot | Info | Host has booted. | `Host has booted.` |  |
| esx.audit.host.crash.reason | Info | Host experienced a crash | `The crash at {1} occurred due to: {2}. More details will be available in the generated vmkernel-zdump.` | <ul><li>{1}: Timestamp</li><li>{2}: Value</li></ul> |
| esx.audit.host.crash.reason.available | Info | The host experienced a crash | `The host experienced a crash. Reason: {1}.` | <ul><li>{1}: Reason</li></ul> |
| esx.audit.host.crash.reason.unavailable | Info | Host experienced a crash | `Host experienced a crash. More details will be available in the generated vmkernel-zdump.` |  |
| esx.audit.host.maxRegisteredVMsExceeded | Warning | The number of virtual machines registered on the host exceeded limit. | `The number of virtual machines registered on host {host.name} in cluster {computeResource.name} in {datacenter.name} exceeded limit: {current} registered, {limit} is the maximum supported.` | <ul><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{current}: current</li><li>{limit}: limit</li></ul> |
| esx.audit.host.poweroff.reason.available | Info | The host has been powered off | `The host has been powered off. Reason for powering off: {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.host.poweroff.reason.management | Info | User had initiated power off | `The power off at {1} was requested by {2} by user/entity {3} due to: {4}.` | <ul><li>{1}: Timestamp</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.audit.host.poweroff.reason.subsystem | Info | User had initiated power off | `The power off at {1} was requested by {2} due to: {3}.` | <ul><li>{1}: Timestamp</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.host.poweroff.reason.timestamp | Info | User had initiated power off | `The power off at {1} was requested due to: {2}.` | <ul><li>{1}: Timestamp</li><li>{2}: Value</li></ul> |
| esx.audit.host.poweroff.reason.unavailable | Info | Host had been powered off | `Host had been powered off. The poweroff was not the result of a kernel error, deliberate reboot, or shut down. This could indicate a hardware issue. Hardware may reboot abruptly due to power outages, faulty components, and heating issues. To investigate further, engage the hardware vendor.` |  |
| esx.audit.host.poweroff.reason.user | Info | User had initiated power off | `The power off at {1} was requested by user/entity {2} due to: {3}.` | <ul><li>{1}: Timestamp</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.host.quickboot.reason.available | Info | The host experienced Quick Boot | `The host experienced Quick Boot. Reason for reboot: {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.host.quickboot.reason.management | Info | User had initiated Quick Boot | `The Quick Boot at {1} was requested by {2} by user/entity {3} due to: {4}.` | <ul><li>{1}: Timestamp</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.audit.host.quickboot.reason.subsystem | Info | User had initiated Quick Boot | `The Quick Boot at {1} was requested by {2} due to: {3}.` | <ul><li>{1}: Timestamp</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.host.quickboot.reason.timestamp | Info | User had initiated Quick Boot | `The Quick Boot at {1} was requested due to: {2}.` | <ul><li>{1}: Timestamp</li><li>{2}: Value</li></ul> |
| esx.audit.host.quickboot.reason.unavailable | Info | Host experienced Quick Boot | `Host experienced Quick Boot. The Quick Boot was not the result of a kernel error, deliberate reboot, or shut down. This could indicate a hardware issue. Hardware may reboot abruptly due to power outages, faulty components, and heating issues. To investigate further, engage the hardware vendor.` |  |
| esx.audit.host.quickboot.reason.user | Info | User had initiated Quick Boot | `The Quick Boot at {1} was requested by user/entity {2} due to: {3}.` | <ul><li>{1}: Timestamp</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.host.reboot.reason.available | Info | The host has been rebooted | `The host has been rebooted. Reason for reboot: {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.host.reboot.reason.management | Info | User had initiated reboot | `The reboot at {1} was requested by {2} by user/entity {3} due to: {4}.` | <ul><li>{1}: Timestamp</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.audit.host.reboot.reason.subsystem | Info | User had initiated reboot | `The reboot at {1} was requested by {2} due to: {3}.` | <ul><li>{1}: Timestamp</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.host.reboot.reason.timestamp | Info | User had initiated reboot | `The reboot at {1} was requested due to: {2}.` | <ul><li>{1}: Timestamp</li><li>{2}: Value</li></ul> |
| esx.audit.host.reboot.reason.unavailable | Info | Host had been rebooted | `Host had been rebooted. The reboot was not the result of a kernel error, deliberate reboot, or shut down. This could indicate a hardware issue. Hardware may reboot abruptly due to power outages, faulty components, and heating issues. To investigate further, engage the hardware vendor.` |  |
| esx.audit.host.reboot.reason.user | Info | User had initiated reboot | `The reboot at {1} was requested by user/entity {2} due to: {3}.` | <ul><li>{1}: Timestamp</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.host.stop.reboot | Info | Host is rebooting. | `Host is rebooting.` |  |
| esx.audit.host.stop.shutdown | Info | Host is shutting down. | `Host is shutting down.` |  |
| esx.audit.hostd.host.poweroff.reason | Warning | Powering off host through hostd | `The host is being powered off through hostd. Reason for powering off: {1}, User: {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.hostd.host.reboot.reason | Warning | Rebooting host through hostd | `The host is being rebooted through hostd. Reason for reboot: {1}, User: {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.hostd.host.restart.reason | Warning | Rebooting host through hostd | `The host is being rebooted through hostd. Reason for reboot: {1}, User: {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.lockdownmode.disabled | Info | Administrator access to the host has been enabled. | `Administrator access to the host has been enabled.` |  |
| esx.audit.lockdownmode.enabled | Info | Administrator access to the host has been disabled. | `Administrator access to the host has been disabled.` |  |
| esx.audit.lockdownmode.exceptions.changed | Info | List of lockdown exception users has been changed. | `List of lockdown exception users has been changed.` |  |
| esx.audit.maintenancemode.canceled | Info | The host has canceled entering maintenance mode. | `The host has canceled entering maintenance mode.` |  |
| esx.audit.maintenancemode.entered | Info | The host has entered maintenance mode. | `The host has entered maintenance mode.` |  |
| esx.audit.maintenancemode.entering | Info | The host has begun entering maintenance mode. | `The host has begun entering maintenance mode.` |  |
| esx.audit.maintenancemode.exited | Info | The host has exited maintenance mode. | `The host has exited maintenance mode.` |  |
| esx.audit.maintenancemode.failed | Error | The host has failed entering maintenance mode. | `The host has failed entering maintenance mode.` |  |
| esx.audit.net.firewall.config.changed | Info | Firewall configuration has changed. | `Firewall configuration has changed. Operation '{1}' for rule set {2} succeeded.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.net.firewall.disabled | Warning | Firewall has been disabled. | `Firewall has been disabled.` |  |
| esx.audit.net.firewall.enabled | Info | Firewall has been enabled for port. | `Firewall has been enabled for port {1}.` | <ul><li>{1}: Port</li></ul> |
| esx.audit.net.firewall.port.hooked | Info | Port is now protected by Firewall. | `Port {1} is now protected by Firewall.` | <ul><li>{1}: Port</li></ul> |
| esx.audit.net.firewall.port.removed | Warning | Port is no longer protected with Firewall. | `Port {1} is no longer protected with Firewall.` | <ul><li>{1}: Port</li></ul> |
| esx.audit.net.lacp.disable | Info | LACP disabled | `LACP for VDS {1} is disabled.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.net.lacp.enable | Info | LACP eabled | `LACP for VDS {1} is enabled.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.net.lacp.uplink.connected | Info | uplink is connected | `LACP info: uplink {1} on VDS {2} got connected.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.partialmaintenancemode.canceled | Info | The host has canceled entering a partial maintenance mode. | `The host has canceled entering '{1}'.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.partialmaintenancemode.entered | Info | The host has entered a partial maintenance mode. | `The host has entered '{1}'.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.partialmaintenancemode.entering | Info | The host has begun entering a partial maintenance mode. | `The host has begun entering '{1}'.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.partialmaintenancemode.exited | Info | The host has exited a partial maintenance mode. | `The host has exited '{1}'.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.partialmaintenancemode.failed | Error | The host has failed entering a partial maintenance mode. | `The host has failed entering '{1}'.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.shell.disabled | Info | The ESXi command line shell has been disabled. | `The ESXi command line shell has been disabled.` |  |
| esx.audit.shell.enabled | Info | The ESXi command line shell has been enabled. | `The ESXi command line shell has been enabled.` |  |
| esx.audit.ssh.disabled | Info | SSH access has been disabled. | `SSH access has been disabled.` |  |
| esx.audit.ssh.enabled | Info | SSH access has been enabled. | `SSH access has been enabled.` |  |
| esx.audit.ssh.session.closed | Info | SSH session was closed. | `SSH session was closed for '{1}@{2}'.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.ssh.session.failed | Info | SSH login has failed. | `SSH login has failed for '{1}@{2}'.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.ssh.session.opened | Info | SSH session was opened. | `SSH session was opened for '{1}@{2}'.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.subsystem.host.poweroff.reason | Warning | Powering off host | `The host is being powered off. Reason for powering off: {1}, User: {2}, Subsystem: {3}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.subsystem.host.reboot.reason | Warning | Rebooting host | `The host is being rebooted. Reason for reboot: {1}, User: {2}, Subsystem: {3}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.subsystem.host.restart.reason | Warning | Rebooting host | `The host is being rebooted. Reason for reboot: {1}, User: {2}, Subsystem: {3}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.audit.supershell.access | Warning | Supershell session has been started by a user. | `Supershell session has been started by a user.` |  |
| esx.audit.test.test1d | Error | Test with an int argument | `Test with {1}` | <ul><li>{1}: Value</li></ul> |
| esx.audit.test.test1s | Error | Test with a string argument | `Test with {1}` | <ul><li>{1}: Value</li></ul> |
| esx.audit.usb.config.changed | Info | USB configuration has changed. | `USB configuration has changed on host {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| esx.audit.uw.secpolicy.alldomains.level.changed | Warning | Enforcement level changed for all security domains. | `The enforcement level for all security domains has been changed to {1}. The enforcement level must always be set to enforcing.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.uw.secpolicy.domain.level.changed | Warning | Enforcement level changed for security domain. | `The enforcement level for security domain {1} has been changed to {2}. The enforcement level must always be set to enforcing.` | <ul><li>{1}: Domain</li><li>{2}: Value</li></ul> |
| esx.audit.uw.security.User.ExecInstalledOnly.disabled | Warning | ExecInstalledOnly has been disabled. This allows the execution of non-installed binaries on the host. Unknown content can cause malware attacks similar to Ransomware. | `ExecInstalledOnly has been disabled. This allows the execution of non-installed binaries on the host. Unknown content can cause malware attacks similar to Ransomware.` |  |
| esx.audit.uw.security.User.ExecInstalledOnly.enabled | Info | ExecInstalledOnly has been enabled. This prevents the execution of non-installed binaries on the host. | `ExecInstalledOnly has been enabled. This prevents the execution of non-installed binaries on the host.` |  |
| esx.audit.uw.security.execInstalledOnly.violation | Warning | Execution of non-installed file prevented. | `Execution of unknown (non VIB installed) binary '{1}' prevented. Unknown content can cause malware attacks similar to Ransomware.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.uw.security.execInstalledOnly.warning | Warning | Execution of non-installed file detected. | `Execution of unknown (non VIB installed) binary '{1}'. Unknown content can cause malware attacks similar to Ransomware.` | <ul><li>{1}: Value</li></ul> |
| esx.audit.vmfs.lvm.device.discovered | Info | LVM device discovered. | `One or more LVM devices have been discovered on this host.` |  |
| esx.audit.vmfs.sesparse.bloomfilter.disabled | Info | Read IO performance maybe impacted for disk | `Read IO performance maybe impacted for disk {1}: {2}` | <ul><li>{1}: Disk</li><li>{2}: Value</li></ul> |
| esx.audit.vmfs.volume.mounted | Info | File system mounted. | `File system {1} on volume {2} has been mounted in {3} mode on this host.` | <ul><li>{1}: File system</li><li>{2}: Volume</li><li>{3}: Value</li></ul> |
| esx.audit.vmfs.volume.umounted | Info | LVM volume un-mounted. | `The volume {1} has been safely un-mounted. The datastore is no longer accessible on this host.` | <ul><li>{1}: Volume</li></ul> |
| esx.audit.vob.vsan.lsom.devicerebuild | Info | vSAN device is added back successfully after MEDIUM error. | `vSAN device {1} is added back successfully after MEDIUM error. Old UUID {2} New UUID {3}.` | <ul><li>{1}: Device ID</li><li>{2}: ID</li><li>{3}: ID</li></ul> |
| esx.audit.vob.vsan.lsom.diskgrouprebuild | Info | vSAN diskgroup is rebuilt successfully after MEDIUM error. | `vSAN diskgroup {1} is rebuilt successfully after MEDIUM error. Old UUID {2} New UUID {3}.` | <ul><li>{1}: Group</li><li>{2}: ID</li><li>{3}: ID</li></ul> |
| esx.audit.vob.vsan.lsom.foundInvalidMetadataComp | Warning | Found components with invalid metadata | `{1} components found with invalid metadata on disk {2} {3}` | <ul><li>{1}: Value</li><li>{2}: Disk</li><li>{3}: Value</li></ul> |
| esx.audit.vob.vsan.lsom.storagepoolrebuild | Info | vSAN storagepool is added back successfully after MEDIUM error. | `vSAN storagepool {1} is added back successfully after MEDIUM error. Old UUID {2} New UUID {3}.` | <ul><li>{1}: Pool</li><li>{2}: ID</li><li>{3}: ID</li></ul> |
| esx.audit.vobdtestcorrelator.test | Info | Test with both int and sting arguments. | `Test with both string: {2} and int: {1}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.audit.vsan.clustering.enabled | Info | vSAN clustering services have been enabled. | `vSAN clustering and directory services have been enabled.` |  |
| esx.audit.vsan.net.vnic.added | Info | vSAN virtual NIC has been added. | `vSAN virtual NIC has been added.` |  |
| esx.audit.vsan.net.vnic.deleted | Error | vSAN network configuration has been removed. | `vSAN network configuration has been removed. The host may experience problems communicating with other hosts in vSAN cluster.` |  |
| esx.audit.vsan.rdma.changed | Info | vSAN RDMA changed for vmknic. | `vSAN RDMA changed for vmknic {1}.` | <ul><li>{1}: Physical NIC</li></ul> |
| esx.audit.weak.ssl.protocol | Warning | Host detected weak SSL protocols and disabled them. Please refer to KB article: KB 2151445 | `Weak SSL protocols found and disabled. Please refer to KB article: KB 1234567` |  |
| esx.clear.bootdevice.apd.exit | Info | Boot device exited the All Paths Down state | `Boot Device with identifier {1} has exited from the state: All Paths Down.` | <ul><li>{1}: Device ID</li></ul> |
| esx.clear.bootdevice.pdl.restored | Info | Connectivity to boot device restored | `Boot Device with identifier {1} is accessible again.` | <ul><li>{1}: Device ID</li></ul> |
| esx.clear.coredump.configured | Info | A vmkcore disk partition is available and/or a network coredump server has been configured.  Host core dumps will be saved. | `A vmkcore disk partition is available and/or a network coredump server has been configured.  Host core dumps will be saved.` |  |
| esx.clear.coredump.configured2 | Info | At least one coredump target has been configured. Host core dumps will be saved. | `At least one coredump target has been configured. Host core dumps will be saved.` |  |
| esx.clear.hardware.nvd.health.module.es.charged | Info | NVDIMM Energy Source is sufficiently charged. | `NVDIMM (handle {1}, idString {2}): Energy Source is sufficiently charged.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.clear.net.connectivity.restored | Info | Restored network connectivity to portgroups | `Network connectivity restored on virtual switch {1}, portgroups: {2}. Physical NIC {3} is up.` | <ul><li>{1}: Virtual switch</li><li>{2}: Portgroup(s)</li><li>{3}: Physical NIC</li></ul> |
| esx.clear.net.dvport.connectivity.restored | Info | Restored Network Connectivity to DVPorts | `Network connectivity restored on DVPorts: {1}. Physical NIC {2} is up.` | <ul><li>{1}: Value</li><li>{2}: Physical NIC</li></ul> |
| esx.clear.net.dvport.redundancy.restored | Info | Restored Network Redundancy to DVPorts | `Uplink redundancy restored on DVPorts: {1}. Physical NIC {2} is up recently.` | <ul><li>{1}: Value</li><li>{2}: Physical NIC</li></ul> |
| esx.clear.net.lacp.lag.transition.up | Info | lag transition up | `LACP info: LAG {1} on VDS {2} is up.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.clear.net.lacp.uplink.transition.up | Info | uplink transition up | `LACP info: uplink {1} on VDS {2} is moved into link aggregation group.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.clear.net.lacp.uplink.unblocked | Info | uplink is unblocked | `LACP info: uplink {1} on VDS {2} is unblocked.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.clear.net.redundancy.restored | Info | Restored uplink redundancy to portgroups | `Uplink redundancy restored on virtual switch {1}, portgroups: {2}. Physical NIC {3} is up.` | <ul><li>{1}: Virtual switch</li><li>{2}: Portgroup(s)</li><li>{3}: Physical NIC</li></ul> |
| esx.clear.net.vmnic.linkstate.up | Info | Link state up | `Physical NIC {1} linkstate is up.` | <ul><li>{1}: Physical NIC</li></ul> |
| esx.clear.psastor.device.io.latency.improved | Info | Storage Device I/O Latency has improved | `Device {1} performance has improved. I/O latency reduced from {2} microseconds to {3} microseconds.` | <ul><li>{1}: Device ID</li><li>{2}: Microseconds</li><li>{3}: Microseconds</li></ul> |
| esx.clear.psastor.device.state.on | Info | Device has been turned on administratively. | `Device {1}, has been turned on administratively.` | <ul><li>{1}: Device ID</li></ul> |
| esx.clear.psastor.device.state.permanentloss.deviceonline | Info | Device that was permanently inaccessible is now online. | `Device {1}, that was permanently inaccessible is now online. No data consistency guarantees.` | <ul><li>{1}: Device ID</li></ul> |
| esx.clear.scsi.device.io.latency.improved | Info | Scsi Device I/O Latency has improved | `Device {1} performance has improved. I/O latency reduced from {2} microseconds to {3} microseconds.` | <ul><li>{1}: Device ID</li><li>{2}: Microseconds</li><li>{3}: Microseconds</li></ul> |
| esx.clear.scsi.device.state.on | Info | Device has been turned on administratively. | `Device {1}, has been turned on administratively.` | <ul><li>{1}: Device ID</li></ul> |
| esx.clear.scsi.device.state.permanentloss.deviceonline | Info | Device that was permanently inaccessible is now online. | `Device {1}, that was permanently inaccessible is now online. No data consistency guarantees.` | <ul><li>{1}: Device ID</li></ul> |
| esx.clear.storage.apd.exit | Info | Exited the All Paths Down state | `Device or filesystem with identifier {1} has exited the All Paths Down state.` | <ul><li>{1}: Identifier</li></ul> |
| esx.clear.storage.connectivity.restored | Info | Restored connectivity to storage device | `Connectivity to storage device {1} (Datastores: {2}) restored. Path {3} is active again.` | <ul><li>{1}: Device ID</li><li>{2}: Datastore(s)</li><li>{3}: Path</li></ul> |
| esx.clear.storage.redundancy.restored | Info | Restored path redundancy to storage device | `Path redundancy to storage device {1} (Datastores: {2}) restored. Path {3} is active again.` | <ul><li>{1}: Device ID</li><li>{2}: Datastore(s)</li><li>{3}: Path</li></ul> |
| esx.clear.vmfs.nfs.server.restored | Info | Restored connection to NFS server | `Restored connection to server {1} mount point {2} mounted as {3} ({4}).` | <ul><li>{1}: Server</li><li>{2}: Mount point</li><li>{3}: Mount name</li><li>{4}: Value</li></ul> |
| esx.clear.vmfs.nfs.volume.io.latency.improved | Info | NFS volume I/O Latency has improved | `NFS volume {1} performance has improved. I/O latency reduced from {2} microseconds to {3} microseconds.` | <ul><li>{1}: Volume</li><li>{2}: Microseconds</li><li>{3}: Microseconds</li></ul> |
| esx.clear.vob.vsan.pdl.online | Info | vSAN device has come online. | `vSAN device {1} has come online.` | <ul><li>{1}: Device ID</li></ul> |
| esx.clear.vobdtestcorrelator.test | Info | Test with both int and sting arguments. | `Test with both string: {1} {3} and int: {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.clear.vsan.clustering.enabled | Info | vSAN clustering services have now been enabled. | `vSAN clustering and directory services have now been enabled.` |  |
| esx.clear.vsan.network.available | Info | vSAN now has at least one active network configuration. | `vSAN now has a usable network configuration. Earlier reported connectivity problems, if any, can now be ignored because they are resolved.` |  |
| esx.clear.vsan.vmknic.ready | Info | A previously reported vmknic now has a valid IP. | `vmknic {1} now has an IP address. Earlier reported connectivity problems, if any, can now be ignored because they are resolved.` | <ul><li>{1}: Physical NIC</li></ul> |
| esx.clear.vvol.container.online | Info | VVol container has come online. | `VVol container {1} has come online.` | <ul><li>{1}: Value</li></ul> |
| esx.clear.vvol.vasaprovider.connectivity.established | Info | Connectivity with the VASA Provider is established. | `Connectivity with the VASA Provider {1} is established.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.3rdParty.error | Error | A 3rd party component on ESXi has reported an error. | `A 3rd party component, {1}, running on ESXi has reported an error. Please follow the knowledge base link ({2}) to see the steps to remedy the problem as reported by {3}. The message reported is: {4}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.problem.3rdParty.info | Info | A 3rd party component on ESXi has reported an informational event. | `A 3rd party component, {1}, running on ESXi has reported an informational event. If needed, please follow the knowledge base link ({2}) to see the steps to remedy the problem as reported by {3}. The message reported is: {4}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.problem.3rdParty.information | Info | A 3rd party component on ESXi has reported an informational event. | `A 3rd party component, {1}, running on ESXi has reported an informational event. If needed, please follow the knowledge base link ({2}) to see the steps to remedy the problem as reported by {3}. The message reported is: {4}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.problem.3rdParty.warning | Warning | A 3rd party component on ESXi has reported a warning. | `A 3rd party component, {1}, running on ESXi has reported a warning related to a problem. Please follow the knowledge base link ({2}) to see the steps to remedy the problem as reported by {3}. The message reported is: {4}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.problem.apei.bert.memory.error.corrected | Error | A corrected memory error occurred | `A corrected memory error occurred in last boot. The following details were reported. Physical Addr: {1}, Physical Addr Mask: {2}, Node: {3}, Card: {4}, Module: {5}, Bank: {6}, Device: {7}, Row: {8}, Column: {9} Error type: {10}` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li><li>{5}: Value</li><li>{6}: Value</li><li>{7}: Value</li><li>{8}: Value</li><li>{9}: Value</li><li>{10}: Value</li></ul> |
| esx.problem.apei.bert.memory.error.fatal | Error | A fatal memory error occurred | `A fatal memory error occurred in the last boot. The following details were reported. Physical Addr: {1}, Physical Addr Mask: {2}, Node: {3}, Card: {4}, Module: {5}, Bank: {6}, Device: {7}, Row: {8}, Column: {9} Error type: {10}` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li><li>{5}: Value</li><li>{6}: Value</li><li>{7}: Value</li><li>{8}: Value</li><li>{9}: Value</li><li>{10}: Value</li></ul> |
| esx.problem.apei.bert.memory.error.recoverable | Error | A recoverable memory error occurred | `A recoverable memory error occurred in last boot. The following details were reported. Physical Addr: {1}, Physical Addr Mask: {2}, Node: {3}, Card: {4}, Module: {5}, Bank: {6}, Device: {7}, Row: {8}, Column: {9} Error type: {10}` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li><li>{5}: Value</li><li>{6}: Value</li><li>{7}: Value</li><li>{8}: Value</li><li>{9}: Value</li><li>{10}: Value</li></ul> |
| esx.problem.apei.bert.pcie.error.corrected | Error | A corrected PCIe error occurred | `A corrected PCIe error occurred in last boot. The following details were reported. Port Type: {1}, Device: {2}, Bus #: {3}, Function: {4}, Slot: {5}, Device Vendor: {6}, Version: {7}, Command Register: {8}, Status Register: {9}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li><li>{5}: Value</li><li>{6}: Value</li><li>{7}: Value</li><li>{8}: Value</li><li>{9}: Value</li></ul> |
| esx.problem.apei.bert.pcie.error.fatal | Error | A fatal PCIe error occurred | `Platform encounterd a fatal PCIe error in last boot. The following details were reported. Port Type: {1}, Device: {2}, Bus #: {3}, Function: {4}, Slot: {5}, Device Vendor: {6}, Version: {7}, Command Register: {8}, Status Register: {9}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li><li>{5}: Value</li><li>{6}: Value</li><li>{7}: Value</li><li>{8}: Value</li><li>{9}: Value</li></ul> |
| esx.problem.apei.bert.pcie.error.recoverable | Error | A recoverable PCIe error occurred | `A recoverable PCIe error occurred in last boot. The following details were reported. Port Type: {1}, Device: {2}, Bus #: {3}, Function: {4}, Slot: {5}, Device Vendor: {6}, Version: {7}, Command Register: {8}, Status Register: {9}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li><li>{5}: Value</li><li>{6}: Value</li><li>{7}: Value</li><li>{8}: Value</li><li>{9}: Value</li></ul> |
| esx.problem.application.core.dumpFailed | Warning | An application running on ESXi host has crashed and core file creation failed. | `An application ({1}) running on ESXi host has crashed ({2} time(s) so far), but core dump creation failed.` | <ul><li>{1}: Value</li><li>{2}: Count</li></ul> |
| esx.problem.application.core.dumped | Warning | An application running on ESXi host has crashed and a core file was created. | `An application ({1}) running on ESXi host has crashed ({2} time(s) so far). A core file might have been created at {3}.` | <ul><li>{1}: Value</li><li>{2}: Count</li><li>{3}: File path</li></ul> |
| esx.problem.application.core.dumped.encrypted | Warning | An application running on ESXi host has crashed and an encrypted core file was created. | `An application ({1}) running on ESXi host has crashed ({2} time(s) so far). An encrypted core file using keyId {3} might have been created at {4}.` | <ul><li>{1}: Value</li><li>{2}: Count</li><li>{3}: ID</li><li>{4}: File path</li></ul> |
| esx.problem.boot.failure.detected | Error | Critical failure detected during boot, please refer to KB 93107. | `A critical failure was detected during system boot. The host cannot currently run workloads. Please refer to KB 93107 for more details.` |  |
| esx.problem.boot.filesystem.down | Error | Lost connectivity to the device backing the boot filesystem | `Lost connectivity to the device {1} backing the boot filesystem {2}. As a result, host configuration changes will not be saved to persistent storage.` | <ul><li>{1}: Device ID</li><li>{2}: Value</li></ul> |
| esx.problem.bootdevice.apd.start | Warning | All paths of boot device are down | `Boot Device with identifier {1} has entered the state: All Paths Down. Host will be halted if the boot device fails to recover in {2} seconds.` | <ul><li>{1}: Device ID</li><li>{2}: Seconds</li></ul> |
| esx.problem.bootdevice.apd.timeout | Error | All Paths Down timed out for boot device | `Boot Device with identifier {1} has entered the state: All Paths Down Timeout. Host will be halted if the boot device fails to recover in {2} seconds.` | <ul><li>{1}: Device ID</li><li>{2}: Seconds</li></ul> |
| esx.problem.bootdevice.monitor.disabled | Warning | Remote boot device monitoring disabled | `Host is not in compliance, remote boot device monitoring is disabled.` |  |
| esx.problem.bootdevice.pdl | Error | Boot device permanent loss | `Boot Device with identifier {1} has entered the state: Permanent Device Loss. Host will be halted if the boot device fails to recover in {2} seconds.` | <ul><li>{1}: Device ID</li><li>{2}: Seconds</li></ul> |
| esx.problem.clock.correction.adjtime.lostsync | Warning | System clock no longer synchronized to upstream time servers | `system clock no longer synchronized to upstream time servers` |  |
| esx.problem.clock.correction.adjtime.sync | Warning | System clock synchronized to upstream time servers | `system clock synchronized to upstream time servers` |  |
| esx.problem.clock.correction.adjtime.unsync | Warning | System clock lost synchronization to upstream time servers | `system clock lost synchronization to upstream time servers` |  |
| esx.problem.clock.correction.changed | Warning | Application system changed clock, synchronization lost | `{1} stepped system clock to {2}.{3}, synchronization lost` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.clock.correction.delta.allowed | Warning | Allowed system clock update with large time change | `Clock stepped to {1}.{2}, but delta {3} > {4} seconds` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Seconds</li></ul> |
| esx.problem.clock.correction.delta.failed | Error | Failed system clock update with large time change | `Clock step to {1}.{2} failed, delta {3} > {4} seconds, number of large corrections > {5}` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Seconds</li><li>{5}: Value</li></ul> |
| esx.problem.clock.correction.delta.warning | Warning | Allowed system clock update with large time change, but number of future updates limited | `Clock stepped to {1}.{2}, but delta {3} > {4} seconds, {5}/{6} large corrections` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Seconds</li><li>{5}: Value</li><li>{6}: Value</li></ul> |
| esx.problem.clock.correction.step.unsync | Warning | System clock stepped, lost synchronization | `system clock stepped to {1}.{2}, lost synchronization` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.clock.parameter.set.maxLargeCorrections | Warning | System clock maximum number of large corrections changed | `system clock max number of correction set to {1}` | <ul><li>{1}: Timestamp (end)</li></ul> |
| esx.problem.clock.parameter.set.maxNegPhaseCorrection | Warning | System clock maximum negative phase correction changed | `system clock max negative phase correction set to {1}` | <ul><li>{1}: Timestamp (end)</li></ul> |
| esx.problem.clock.parameter.set.maxPosPhaseCorrection | Warning | System clock maximum positive phase correction changed | `system clock max positive phase correction set to {1}` | <ul><li>{1}: Timestamp (end)</li></ul> |
| esx.problem.clock.parameter.set.numLargeCorrections | Warning | System clock count of number of large corrections changed | `system clock number of large correction set to {1}` | <ul><li>{1}: Timestamp (end)</li></ul> |
| esx.problem.clock.parameter.set.vobReportInterval | Warning | System clock VOB report interval changed | `system clock max number of correction set to {1}` | <ul><li>{1}: Timestamp (end)</li></ul> |
| esx.problem.clock.state.reset | Warning | System clock state has been reset | `system clock state has been reset` |  |
| esx.problem.coredump.capacity.insufficient | Warning | The storage capacity of the coredump targets is insufficient to capture a complete coredump. | `The storage capacity of the coredump targets is insufficient to capture a complete coredump. Recommended coredump capacity is {1} MiB.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.coredump.copyspace | Warning | The free space available in default coredump copy location is insufficient to copy new coredumps. | `The free space available in default coredump copy location is insufficient to copy new coredumps. Recommended free space is {1} MiB.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.coredump.extraction.failed.nospace | Warning | The given partition has insufficient amount of free space to extract the coredump. | `The given partition has insufficient amount of free space to extract the coredump. At least {1} MiB is required.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.coredump.unconfigured | Warning | No vmkcore disk partition is available and no network coredump server has been configured.  Host core dumps cannot be saved. | `No vmkcore disk partition is available and no network coredump server has been configured.  Host core dumps cannot be saved.` |  |
| esx.problem.coredump.unconfigured2 | Warning | No coredump target has been configured. Host core dumps cannot be saved. | `No coredump target has been configured. Host core dumps cannot be saved.` |  |
| esx.problem.cpu.amd.mce.dram.disabled | Error | DRAM ECC not enabled. Please enable it in BIOS. | `DRAM ECC not enabled. Please enable it in BIOS.` |  |
| esx.problem.cpu.intel.ioapic.listing.error | Error | Not all IO-APICs are listed in the DMAR. Not enabling interrupt remapping on this platform.  | `Not all IO-APICs are listed in the DMAR. Not enabling interrupt remapping on this platform. ` |  |
| esx.problem.cpu.mce.invalid | Error | MCE monitoring will be disabled as an unsupported CPU was detected. Please consult the ESX HCL for information on supported hardware. | `MCE monitoring will be disabled as an unsupported CPU was detected. Please consult the ESX HCL for information on supported hardware.` |  |
| esx.problem.cpu.page.correctederrors.high | Info | High number of corrected errors on a page. | `High number of corrected errors on host physical page number {1}` | <ul><li>{1}: Value</li></ul> |
| esx.problem.cpu.smp.ht.invalid | Error | Disabling HyperThreading due to invalid configuration: Number of threads: {1}, Number of PCPUs: {2}. | `Disabling HyperThreading due to invalid configuration: Number of threads: {1}, Number of PCPUs: {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.cpu.smp.ht.numpcpus.max | Error | Found {1} PCPUs, but only using {2} of them due to specified limit. | `Found {1} PCPUs, but only using {2} of them due to specified limit.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.cpu.smp.ht.partner.missing | Error | Disabling HyperThreading due to invalid configuration: HT partner {1} is missing from PCPU {2}. | `Disabling HyperThreading due to invalid configuration: HT partner {1} is missing from PCPU {2}.` | <ul><li>{1}: Value</li><li>{2}: Physical CPU</li></ul> |
| esx.problem.cs.createstore.copy.backup.error | Error | Error copying ConfigStore from backup. | `Error copying ConfigStore from backup {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.cs.db.operation.error | Error | Failed an operation on the ConfigStore database. | `Failed an operation on the ConfigStore database.` |  |
| esx.problem.cs.desired.config.error | Error | Failed to setup desired configuration. | `Failed to setup desired configuration.` |  |
| esx.problem.cs.dfs.cleanup.error | Error | Error cleaning up Datafile store. | `Error cleaning up Datafile store.` |  |
| esx.problem.cs.dfs.restore.error | Error | DataFile store cannot be restored. | `DataFile store cannot be restored.` |  |
| esx.problem.cs.schema.file.error | Error | Error processing schema file. | `Error processing schema file {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.cs.schema.metadata.error | Error | Invalid metadata in schema file. | `Invalid metadata in schema file {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.cs.schema.validation.error | Error | VibId validation failed for schema file. | `VibId validation failed for schema file {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.cs.upgrade.config.error | Error | Error in upgrading config. | `Error in upgrading config {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.dhclient.lease.none | Error | Unable to obtain a DHCP lease. | `Unable to obtain a DHCP lease on interface {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.dhclient.lease.offered.error | Error | No expiry time on offered DHCP lease. | `No expiry time on offered DHCP lease from {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.dhclient.lease.offered.noexpiry | Error | No expiry time on offered DHCP lease. | `No expiry time on offered DHCP lease from {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.dpu.maintenance.sync.failed | Warning | The maintenance mode state for some Data Processing Units may be out of sync with the host. | `The maintenance mode state for Data Processing Units with ids '{dpus}' may be out of sync with the host.` | <ul><li>{dpus}: dpus</li></ul> |
| esx.problem.driver.abnormal | Warning | Some drivers need special notice. | `Driver for device {1} is {2}. Please refer to KB article: {3}.` | <ul><li>{1}: Device ID</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.entropy.config.error | Error | Host is configured with external entropy source. Entropy daemon has become non functional because of cache size change. Please refer to KB 89074 for more details. | `Host is configured with external entropy source. Entropy daemon has become non functional because of an {1} change. Please refer to KB 89074 for more details.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.entropy.empty | Error | Host is configured with external entropy source. The entropy available in the memory cache and storage cache is exhausted. Please refer to KB 89074 for more details. | `Host is configured with external entropy source. The entropy available in the memory cache and storage cache is exhausted. Please refer to KB 89074 for more details.` |  |
| esx.problem.entropy.inmemory.empty | Error | Host is configured with external entropy source. The entropy available in the memory cache is exhausted. Please refer to KB 89074 for more details. | `Host is configured with external entropy source. The entropy available in the memory cache is exhausted. Please refer to KB 89074 for more details.` |  |
| esx.problem.esximage.install.error | Error | Could not install image profile. | `Could not install image profile: {1}` | <ul><li>{1}: Value</li></ul> |
| esx.problem.esximage.install.invalidhardware | Error | Host doesn't meet image profile hardware requirements. | `Host doesn't meet image profile '{1}' hardware requirements: {2}` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.esximage.install.stage.error | Error | Could not stage image profile. | `Could not stage image profile '{1}': {2}` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.evc.incompatible | Warning | The host can not support the applied EVC mode. | `The host can not support the applied EVC mode.` |  |
| esx.problem.hardware.acpi.interrupt.routing.device.invalid | Error | Skipping interrupt routing entry with bad device number: {1}. This is a BIOS bug. | `Skipping interrupt routing entry with bad device number: {1}. This is a BIOS bug.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.hardware.acpi.interrupt.routing.pin.invalid | Error | Skipping interrupt routing entry with bad device pin: {1}. This is a BIOS bug. | `Skipping interrupt routing entry with bad device pin: {1}. This is a BIOS bug.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.congestion.clear | Info | FPIN FC congestion clear: Host WWPN {1}, target WWPN {2}. | `FPIN FC congestion clear: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.congestion.creditstall | Warning | FPIN FC credit stall congestion: Host WWPN {1}, target WWPN {2}. | `FPIN FC credit stall congestion: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.congestion.devicespecific | Info | FPIN FC device specific congestion: Host WWPN {1}, target WWPN {2}. | `FPIN FC device specific congestion: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.congestion.lostcredit | Warning | FPIN FC lost credit congestion: Host WWPN {1}, target WWPN {2}. | `FPIN FC lost credit congestion: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.congestion.oversubscription | Warning | FPIN FC oversubscription congestion: Host WWPN {1}, target WWPN {2}. | `FPIN FC oversubscription congestion: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.delivery.devicespecific | Info | FPIN FC device specific delivery notification: Host WWPN {1}, target WWPN {2}. | `FPIN FC device specific delivery notification: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.delivery.timeout | Warning | FPIN FC delivery time out: Host WWPN {1}, target WWPN {2}. | `FPIN FC delivery time out: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.delivery.unabletoroute | Warning | FPIN FC delivery unable to route: Host WWPN {1}, target WWPN {2}. | `FPIN FC delivery unable to route: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.delivery.unknown | Info | FPIN FC unknown delivery notification: Host WWPN {1}, target WWPN {2}. | `FPIN FC unknown delivery notification: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.linkintegrity.devicespecific | Info | FPIN FC device specific link integrity notification: Host WWPN {1}, target WWPN {2}. | `FPIN FC device specific link integrity notification: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.linkintegrity.invalidCRC | Warning | FPIN FC link invalid CRC: Host WWPN {1}, target WWPN {2}. | `FPIN FC link invalid CRC: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.linkintegrity.invalidtransmissionword | Warning | FPIN FC link invalid transmission word: Host WWPN {1}, target WWPN {2}. | `FPIN FC link invalid transmission word: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.linkintegrity.linkfailure | Warning | FPIN FC link failure: Host WWPN {1}, target WWPN {2}. | `FPIN FC link failure: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.linkintegrity.lossofsignal | Warning | FPIN FC link loss of signal: Host WWPN {1}, target WWPN {2}. | `FPIN FC link loss of signal: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.linkintegrity.lossofsynchronization | Warning | FPIN FC link loss of synchronization: Host WWPN {1}, target WWPN {2}. | `FPIN FC link loss of synchronization: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.linkintegrity.primitivesequenceprotocolerror | Warning | FPIN FC link primitive sequence protocol error: Host WWPN {1}, target WWPN {2}. | `FPIN FC link primitive sequence protocol error: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.linkintegrity.uncorrectableFECerror | Warning | FPIN FC link uncorrectable FEC error: Host WWPN {1}, target WWPN {2}. | `FPIN FC link uncorrectable FEC error: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.linkintegrity.unknown | Info | FPIN FC unknown link integrity notification: Host WWPN {1}, target WWPN {2}. | `FPIN FC unknown link integrity notification: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.peercongestion.clear | Info | FPIN FC peer congestion clear: Host WWPN {1}, target WWPN {2}. | `FPIN FC peer congestion clear: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.peercongestion.creditstall | Warning | FPIN FC credit stall peer congestion: Host WWPN {1}, target WWPN {2}. | `FPIN FC credit stall peer congestion: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.peercongestion.devicespecific | Info | FPIN FC device specific peer congestion: Host WWPN {1}, target WWPN {2}. | `FPIN FC device specific peer congestion: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.peercongestion.lostcredit | Warning | FPIN FC lost credit peer congestion: Host WWPN {1}, target WWPN {2}. | `FPIN FC lost credit peer congestion: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.fpin.fc.peercongestion.oversubscription | Warning | FPIN FC oversubscription peer congestion: Host WWPN {1}, target WWPN {2}. | `FPIN FC oversubscription peer congestion: Host WWPN {1}, target WWPN {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.ioapic.missing | Error | IOAPIC Num {1} is missing. Please check BIOS settings to enable this IOAPIC. | `IOAPIC Num {1} is missing. Please check BIOS settings to enable this IOAPIC.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.hardware.ipmi.bmc.bad | Error | Failed to communicate with the BMC. IPMI functionality will be unavailable on this system. | `Failed to communicate with the BMC. IPMI functionality will be unavailable on this system.` |  |
| esx.problem.hardware.nvd.health.alarms.es.lifetime.error | Error | NVDIMM: Energy Source Lifetime Error tripped. | `NVDIMM (handle {1}, idString {2}): Energy Source Lifetime ({3}) Error tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.hardware.nvd.health.alarms.es.temperature.error | Error | NVDIMM: Energy Source Temperature Error tripped. | `NVDIMM (handle {1}, idString {2}): Energy Source Temperature ({3} C) Error tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.hardware.nvd.health.alarms.lifetime.error | Error | NVDIMM: Lifetime Error tripped. | `NVDIMM (handle {1}, idString {2}): Lifetime ({3}) Error tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.hardware.nvd.health.lastshutdownstatus | Error | NVDIMM (handle {1}, idString {2}): Last Shutdown Status ({3}) Not a clean Shutdown, there was either a platform or memory device-related failure while saving data targeted for this memory device. | `NVDIMM (handle {1}, idString {2}): Last Shutdown Status ({3}) Not a clean Shutdown, there was either a platform or memory device-related failure while saving data targeted for this memory device.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.hardware.nvd.health.module.config.error | Error | NVDIMM Configuration error detected. | `NVDIMM (handle {1}, idString {2}): Configuration error detected.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.module.ctlr.fail | Error | NVDIMM Controller failure detected. | `NVDIMM (handle {1}, idString {2}): Controller failure detected. Access to the device and its capabilities are lost.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.module.ctlr.fw.error | Error | NVDIMM Controller firmware error detected. | `NVDIMM (handle {1}, idString {2}): Controller firmware error detected.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.module.es.charging | Warning | NVDIMM Energy Source still charging. | `NVDIMM (handle {1}, idString {2}): Energy Source still charging but does not have sufficient charge to support a backup. Persistency is temporarily lost for the device.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.module.es.fail | Error | NVDIMM Energy Source failure detected. | `NVDIMM (handle {1}, idString {2}): Energy Source failure detected. Persistency is lost for the device.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.module.ops.arm.fail | Warning | NVDIMM Previous ARM operation failed. | `NVDIMM (handle {1}, idString {2}): Previous ARM operation failed.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.module.ops.erase.fail | Warning | NVDIMM Previous ERASE operation failed. | `NVDIMM (handle {1}, idString {2}): Previous ERASE operation failed.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.module.ops.flush.fail | Error | The Platform flush failed. The restored data may be inconsistent. | `NVDIMM (handle {1}, idString {2}): The Platform flush failed. The restored data may be inconsistent.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.module.ops.restore.fail | Error | NVDIMM Last RESTORE operation failed. | `NVDIMM (handle {1}, idString {2}): Last RESTORE operation failed.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.module.ops.save.fail | Error | NVDIMM Previous SAVE operation failed. | `NVDIMM (handle {1}, idString {2}): Previous SAVE operation failed.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.module.uce | Warning | NVDIMM Count of DRAM uncorrectable ECC errors above threshold. | `NVDIMM (handle {1}, idString {2}): Count of DRAM uncorrectable ECC errors above threshold.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.module.vendor.error | Error | NVDIMM Vendor specific error. | `NVDIMM (handle {1}, idString {2}): Vendor specific error.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.vmw.alarms.es.lifetime.error | Error | NVDIMM: Energy Source Lifetime Error tripped. | `NVDIMM (handle {1}, idString {2}): Energy Source Lifetime Error tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.vmw.alarms.es.temperature.error | Error | NVDIMM: Energy Source Temperature Error tripped. | `NVDIMM (handle {1}, idString {2}): Energy Source Temperature Error tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.vmw.alarms.module.lifetime.error | Error | NVDIMM: Module Lifetime Error tripped. | `NVDIMM (handle {1}, idString {2}): Module Lifetime Error tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.vmw.alarms.module.temperature.error | Error | NVDIMM: Module Temperature Error tripped. | `NVDIMM (handle {1}, idString {2}): Module Temperature Error tripped.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.vmw.statusflags.allDataLossInPowerLoss | Error | NVDIMM: All data may be lost in the event of power loss. | `NVDIMM (handle {1}, idString {2}): All data may be lost in the event of power loss.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.vmw.statusflags.allDataLossInShutdown | Error | NVDIMM: All data may be lost in the event of shutdown. | `NVDIMM (handle {1}, idString {2}): All data may be lost in the event of shutdown.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.vmw.statusflags.allDataLossNow | Error | NVDIMM: Subsequent reads may fail or return invalid data and subsequent writes may not persist. | `NVDIMM (handle {1}, idString {2}): Subsequent reads may fail or return invalid data and subsequent writes may not persist.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.vmw.statusflags.perfDegraded | Error | NVDIMM: Performance degraded. | `NVDIMM (handle {1}, idString {2}): Performance degraded.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.vmw.statusflags.wpLossInPowerLoss | Error | NVDIMM: Write persistency loss may happen in event of power loss. | `NVDIMM (handle {1}, idString {2}): Write persistency loss may happen in event of power loss.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.vmw.statusflags.wpLossInShutdown | Error | NVDIMM: Write persistency loss may happen in event of shutdown. | `NVDIMM (handle {1}, idString {2}): Write persistency loss may happen in event of shutdown.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.nvd.health.vmw.statusflags.wpLossNow | Error | NVDIMM: Subsequent writes may not persist. | `NVDIMM (handle {1}, idString {2}): Subsequent writes may not persist.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.hardware.tpm2.connection | Warning | TPM 2.0 device detected but a connection cannot be established. | `TPM 2.0 device detected but a connection cannot be established.` |  |
| esx.problem.hardware.tpm2.ek.provisioning | Warning | Unable to provision Endorsement Key on TPM 2.0 device. | `Unable to provision Endorsement Key on TPM 2.0 device: {1}` | <ul><li>{1}: Value</li></ul> |
| esx.problem.hardware.tpm2.nosha256 | Error | TPM 2.0 SHA-256 PCR bank not found to be active. Please activate it in the BIOS. | `TPM 2.0 SHA-256 PCR bank not found to be active. Please activate it in the BIOS.` |  |
| esx.problem.hardware.tpm2.notis | Error | TPM 2.0 device does not have the TIS interface active. Please activate it in the BIOS. | `TPM 2.0 device does not have the TIS interface active. Please activate it in the BIOS.` |  |
| esx.problem.hardware.tpm2.ownership | Warning | Unable to acquire ownership of TPM 2.0 device. Please clear TPM through the BIOS. | `Unable to acquire ownership of TPM 2.0 device. Please clear TPM through the BIOS.` |  |
| esx.problem.hcm.event.disk.predictive.failure | Warning | A physical disk has a predictive failure. | `A physical disk has a predictive failure ({1}).` | <ul><li>{1}: Value</li></ul> |
| esx.problem.host.coredump | Warning | An unread host kernel core dump has been found. | `An unread host kernel core dump has been found.` |  |
| esx.problem.hostd.core.dumped | Warning | Hostd crashed and a core file was created. | `{1} crashed ({2} time(s) so far) and a core file might have been created at {3}. This might have caused connections to the host to be dropped.` | <ul><li>{1}: Value</li><li>{2}: Count</li><li>{3}: File path</li></ul> |
| esx.problem.hostd.core.dumped.encrypted | Warning | Hostd crashed and an encrypted core file was created. | `{1} crashed ({2} time(s) so far) and an encrypted core file using keyId {3} might have been created at {4}. This might have caused connections to the host to be dropped.` | <ul><li>{1}: Value</li><li>{2}: Count</li><li>{3}: ID</li><li>{4}: File path</li></ul> |
| esx.problem.hyperthreading.unmitigated | Info | This host is potentially vulnerable to issues described in CVE-2018-3646, please refer to https://kb.vmware.com/s/article/55636 for details and VMware recommendations. | `This host is potentially vulnerable to issues described in CVE-2018-3646, please refer to https://kb.vmware.com/s/article/55636 for details and VMware recommendations.` |  |
| esx.problem.inventory.invalidConfigEntries | Warning | Some of the config entries in the VM inventory were skipped because they are invalid. | `Some of the config entries in the VM inventory were skipped because they are invalid.` |  |
| esx.problem.iofilter.disabled | Error | An iofilter installed on the host has stopped functioning. | `IOFilter {1} has stopped functioning due to an unrecoverable error. Reason: {2}` | <ul><li>{1}: Value</li><li>{2}: Reason</li></ul> |
| esx.problem.iorm.badversion | Info | Storage I/O Control version mismatch | `Host {1} cannot participate in Storage I/O Control(SIOC) on datastore {2} because the version number {3} of the SIOC agent on this host is incompatible with number {4} of its counterparts on other hosts connected to this datastore.` | <ul><li>{1}: Host name</li><li>{2}: Datastore(s)</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.problem.iorm.nonviworkload | Info | Unmanaged workload detected on SIOC-enabled datastore | `An unmanaged I/O workload is detected on a SIOC-enabled datastore: {1}.` | <ul><li>{1}: Datastore(s)</li></ul> |
| esx.problem.livePatch.daemon.fallback | Error | Live Patch - daemon remediation fallback failed | `Live Patch - daemon remediation fallback failed for '{1}'. Please refer to the KB 375947 for more details.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.livePatch.daemon.start | Error | Live Patch - failed to start patched daemon | `Live Patch - failed to start patched daemon '{1}'. Please refer to the KB 375947 for more details.` | <ul><li>{1}: Daemon name</li></ul> |
| esx.problem.livePatch.daemon.stop | Error | Live Patch - failed to stop daemon for remediation | `Live Patch - failed to stop daemon '{1}' for remediation. Please refer to the KB 375947 for more details.` | <ul><li>{1}: Daemon name</li></ul> |
| esx.problem.metadatastore.degraded | Error | The metadata store has degraded on one of the hosts in the cluster. | `The metadata store has degraded on host {1}.` | <ul><li>{1}: Host name</li></ul> |
| esx.problem.metadatastore.healthy | Info | The metadata store is healthy. | `The metadata store is healthy.` |  |
| esx.problem.migrate.vmotion.default.heap.create.failed | Warning | Failed to create default migration heap | `Failed to create default migration heap. This might be the result of severe host memory pressure or virtual address space exhaustion. Migration might still be possible, but will be unreliable in cases of extreme host memory pressure.` |  |
| esx.problem.migrate.vmotion.server.pending.cnx.listen.socket.shutdown | Error | Error with migration listen socket | `The ESXi host's vMotion network server encountered an error while monitoring incoming network connections. Shutting down listener socket. vMotion might not be possible with this host until vMotion is manually re-enabled. Failure status: {1}` | <ul><li>{1}: Status</li></ul> |
| esx.problem.module.maxvfs.set | Warning | The max_vfs module option has been set for at least one module. | `Setting the max_vfs option for module {1} may not work as expected. It may be overridden by per-device SRIOV configuration.` | <ul><li>{1}: Module</li></ul> |
| esx.problem.net.connectivity.lost | Error | Lost Network Connectivity | `Lost network connectivity on virtual switch {1}. Physical NIC {2} is down. Affected portgroups:{3}.` | <ul><li>{1}: Virtual switch</li><li>{2}: Physical NIC</li><li>{3}: Portgroup(s)</li></ul> |
| esx.problem.net.dvport.connectivity.lost | Error | Lost Network Connectivity to DVPorts | `Lost network connectivity on DVPorts: {1}. Physical NIC {2} is down.` | <ul><li>{1}: Value</li><li>{2}: Physical NIC</li></ul> |
| esx.problem.net.dvport.redundancy.degraded | Warning | Network Redundancy Degraded on DVPorts | `Uplink redundancy degraded on DVPorts: {1}. Physical NIC {2} is down.` | <ul><li>{1}: Value</li><li>{2}: Physical NIC</li></ul> |
| esx.problem.net.dvport.redundancy.lost | Warning | Lost Network Redundancy on DVPorts | `Lost uplink redundancy on DVPorts: {1}. Physical NIC {2} is down.` | <ul><li>{1}: Value</li><li>{2}: Physical NIC</li></ul> |
| esx.problem.net.e1000.tso6.notsupported | Error | No IPv6 TSO support | `Guest-initiated IPv6 TCP Segmentation Offload (TSO) packets ignored. Manually disable TSO inside the guest operating system in virtual machine {1}, or use a different virtual adapter.` | <ul><li>{1}: VM name</li></ul> |
| esx.problem.net.fence.port.badfenceid | Error | Invalid fenceId configuration on dvPort | `VMkernel failed to set fenceId {1} on distributed virtual port {2} on switch {3}. Reason: invalid fenceId.` | <ul><li>{1}: ID</li><li>{2}: Port</li><li>{3}: Virtual switch</li></ul> |
| esx.problem.net.fence.resource.limited | Error | Maximum number of fence networks or ports | `Vmkernel failed to set fenceId {1} on distributed virtual port {2} on switch {3}. Reason: maximum number of fence networks or ports have been reached.` | <ul><li>{1}: ID</li><li>{2}: Port</li><li>{3}: Virtual switch</li></ul> |
| esx.problem.net.fence.switch.unavailable | Error | Switch fence property is not set | `Vmkernel failed to set fenceId {1} on distributed virtual port {2} on switch {3}. Reason: dvSwitch fence property is not set.` | <ul><li>{1}: ID</li><li>{2}: Port</li><li>{3}: Virtual switch</li></ul> |
| esx.problem.net.firewall.config.failed | Error | Firewall configuration operation failed. The changes were not applied. | `Firewall configuration operation '{1}' failed. The changes were not applied to rule set {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.net.firewall.port.hookfailed | Error | Adding port to Firewall failed. | `Adding port {1} to Firewall failed.` | <ul><li>{1}: Port</li></ul> |
| esx.problem.net.gateway.set.failed | Error | Failed to set gateway | `Cannot connect to the specified gateway {1}. Failed to set it.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.net.heap.belowthreshold | Warning | Network memory pool threshold | `{1} free size dropped below {2} percent.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.net.lacp.lag.transition.down | Warning | lag transition down | `LACP warning: LAG {1} on VDS {2} is down.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.net.lacp.peer.noresponse | Error | No peer response | `LACP error: No peer response on uplink {1} for VDS {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.net.lacp.peer.noresponse.2 | Error | No peer response | `LACP error: No peer response on VDS {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.net.lacp.policy.incompatible | Error | Current teaming policy is incompatible | `LACP error: Current teaming policy on VDS {1} is incompatible, supported is IP hash only.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.net.lacp.policy.linkstatus | Error | Current teaming policy is incompatible | `LACP error: Current teaming policy on VDS {1} is incompatible, supported link failover detection is link status only.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.net.lacp.uplink.blocked | Warning | uplink is blocked | `LACP warning: uplink {1} on VDS {2} is blocked.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.net.lacp.uplink.disconnected | Warning | uplink is disconnected | `LACP warning: uplink {1} on VDS {2} got disconnected.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.net.lacp.uplink.fail.duplex | Error | uplink duplex mode is different | `LACP error: Duplex mode across all uplink ports must be full, VDS {1} uplink {2} has different mode.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.net.lacp.uplink.fail.speed | Error | uplink speed is different | `LACP error: Speed across all uplink ports must be same, VDS {1} uplink {2} has different speed.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.net.lacp.uplink.inactive | Error | All uplinks must be active | `LACP error: All uplinks on VDS {1} must be active.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.net.lacp.uplink.transition.down | Warning | uplink transition down | `LACP warning: uplink {1} on VDS {2} is moved out of link aggregation group.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.net.migrate.bindtovmk | Warning | Invalid vmknic specified in /Migrate/Vmknic | `The ESX advanced configuration option /Migrate/Vmknic is set to an invalid vmknic: {1}.  /Migrate/Vmknic specifies a vmknic that vMotion binds to for improved performance. Update the configuration option with a valid vmknic. Alternatively, if you do not want vMotion to bind to a specific vmknic, remove the invalid vmknic and leave the option blank.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.net.migrate.unsupported.latency | Warning | Unsupported vMotion network latency detected | `ESXi has detected {1}ms round-trip vMotion network latency between host {2} and {3}.  High latency vMotion networks are supported only if both ESXi hosts have been configured for vMotion latency tolerance.` | <ul><li>{1}: Milliseconds</li><li>{2}: Host name</li><li>{3}: Host name</li></ul> |
| esx.problem.net.portset.port.full | Error | Failed to apply for free ports | `Portset {1} has reached the maximum number of ports ({2}). Cannot apply for any more free ports.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.net.portset.port.vlan.invalidid | Error | Vlan ID of the port is invalid | `{1} VLANID {2} is invalid. VLAN ID must be between 0 and 4095.` | <ul><li>{1}: Value</li><li>{2}: ID</li></ul> |
| esx.problem.net.portset.unsupported.psclass | Warning | Try to register an unsupported portset class | `{1} is not a VMware supported portset class, the relevant module must be unloaded.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.net.proxyswitch.port.unavailable | Warning | Virtual NIC connection to switch failed | `Virtual NIC with hardware address {1} failed to connect to distributed virtual port {2} on switch {3}. There are no more ports available on the host proxy switch.` | <ul><li>{1}: MAC address</li><li>{2}: Port</li><li>{3}: Virtual switch</li></ul> |
| esx.problem.net.redundancy.degraded | Warning | Network Redundancy Degraded | `Uplink redundancy degraded on virtual switch {1}. Physical NIC {2} is down. Affected portgroups:{3}.` | <ul><li>{1}: Virtual switch</li><li>{2}: Physical NIC</li><li>{3}: Portgroup(s)</li></ul> |
| esx.problem.net.redundancy.lost | Warning | Lost Network Redundancy | `Lost uplink redundancy on virtual switch {1}. Physical NIC {2} is down. Affected portgroups:{3}.` | <ul><li>{1}: Virtual switch</li><li>{2}: Physical NIC</li><li>{3}: Portgroup(s)</li></ul> |
| esx.problem.net.rspan.teaming.uplink.io.conflict | Error | RSPAN src session conflict with teaming | `Failed to set RSPAN src session {1} on portset {2} due to it disallows uplink I/O which conflicts with {3} teaming policy {4}.` | <ul><li>{1}: Session</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Policy</li></ul> |
| esx.problem.net.teaming.policy.invalid.uplink | Error | The teaming policy has an invalid uplink | `Failed to update teaming policy {1} on portset {2} due to an invalid uplink {3} which disallows normal I/O.` | <ul><li>{1}: Policy</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.net.uplink.mtu.failed | Warning | Failed to set MTU on an uplink | `VMkernel failed to set the MTU value {1} on the uplink {2}.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.net.vmknic.ip.duplicate | Warning | A duplicate IP address was detected on a vmknic interface | `A duplicate IP address was detected for {1} on the interface {2}. The current owner is {3}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.net.vmnic.linkstate.down | Warning | Link state down | `Physical NIC {1} linkstate is down.` | <ul><li>{1}: Physical NIC</li></ul> |
| esx.problem.net.vmnic.linkstate.flapping | Warning | Link state unstable | `Taking down physical NIC {1} because the link is unstable.` | <ul><li>{1}: Physical NIC</li></ul> |
| esx.problem.net.vmnic.watchdog.reset | Warning | Nic Watchdog Reset | `Uplink {1} has recovered from a transient failure due to watchdog timeout` | <ul><li>{1}: Value</li></ul> |
| esx.problem.ntpd.clock.correction.error | Error | NTP daemon stopped.  Time correction out of bounds. | `NTP daemon stopped.  Time correction {1} > {2} seconds.   Manually set the time and restart ntpd.` | <ul><li>{1}: Value</li><li>{2}: Seconds</li></ul> |
| esx.problem.osdata.partition.full | Warning | OSData is low on available space ({1} MiB free). This may result in system failure. Please refer to KB article: KB 87212 | `OSData is low on available space ({1} MiB free). This may result in system failure. Please refer to KB article: KB 87212` | <ul><li>{1}: Value</li></ul> |
| esx.problem.osdata.path.notfound | Warning | Configured OSData cannot be found. Please refer to KB article: KB 87212. | `Configured OSData cannot be found. Please refer to KB article: KB 87212.` |  |
| esx.problem.pageretire.mce.injected | Error | Virtual machine killed as it kept using a corrupted memory page. | `Killing virtual machine with config path {1} because at least {2} uncorrectable memory error machine check exceptions were injected for guest physical page {3} but the virtual machine's operating system kept using the page.` | <ul><li>{1}: Path</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.pageretire.mce.injected.2 | Error | A virtual machine was killed as it kept using a corrupted memory page. | `{1} was killed as it kept using a corrupted memory page {3} even though {2} uncorrectable memory machine check exceptions were injected.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.pageretire.platform.retire.request | Info | Memory page retirement requested by platform firmware. | `Memory page retirement requested by platform firmware. FRU ID: {1}. Refer to System Hardware Log: {2}` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.pageretire.selectedbutnotretired.high | Warning | Number of host physical memory pages that have been selected for retirement but could not yet be retired is high. | `Number of host physical memory pages that have been selected for retirement but could not yet be retired is high: ({1})` | <ul><li>{1}: Value</li></ul> |
| esx.problem.pageretire.selectedmpnthreshold.host.exceeded | Warning | Number of host physical memory pages selected for retirement exceeds threshold. | `Number of host physical memory pages that have been selected for retirement ({1}) exceeds threshold ({2}).` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.psastor.apd.event.descriptor.alloc.failed | Warning | No memory to allocate APD Event | `No memory to allocate APD (All Paths Down) event subsystem.` |  |
| esx.problem.psastor.device.close.failed | Warning | Storage Device close failed. | `"Failed to close the device {1} properly, plugin {2}.` | <ul><li>{1}: Device ID</li><li>{2}: Plugin</li></ul> |
| esx.problem.psastor.device.detach.failed | Warning | Device detach failed | `Detach failed for device :{1}. Exceeded the number of devices that can be detached, please cleanup stale detach entries.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.psastor.device.io.bad.plugin.type | Warning | Plugin trying to issue command to device does not have a valid storage plugin type. | `Bad plugin type for device {1}, plugin {2}` | <ul><li>{1}: Device ID</li><li>{2}: Plugin</li></ul> |
| esx.problem.psastor.device.io.latency.high | Warning | Storage Device I/O Latency going high | `Device {1} performance has deteriorated. I/O latency increased from average value of {2} microseconds to {3} microseconds.` | <ul><li>{1}: Device ID</li><li>{2}: Microseconds</li><li>{3}: Microseconds</li></ul> |
| esx.problem.psastor.device.is.local.failed | Warning | Plugin's isLocal entry point failed | `Failed to verify if the device {1} from plugin {2} is a local - not shared - device` | <ul><li>{1}: Device ID</li><li>{2}: Plugin</li></ul> |
| esx.problem.psastor.device.is.pseudo.failed | Warning | Plugin's isPseudo entry point failed | `Failed to verify if the device {1} from plugin {2} is a pseudo device` | <ul><li>{1}: Device ID</li><li>{2}: Plugin</li></ul> |
| esx.problem.psastor.device.is.ssd.failed | Warning | Plugin's isSSD entry point failed | `Failed to verify if the device {1} from plugin {2} is a Solid State Disk device` | <ul><li>{1}: Device ID</li><li>{2}: Plugin</li></ul> |
| esx.problem.psastor.device.limitreached | Error | Maximum number of storage devices | `The maximum number of supported devices of {1} has been reached. A device from plugin {2} could not be created.` | <ul><li>{1}: Value</li><li>{2}: Plugin</li></ul> |
| esx.problem.psastor.device.state.off | Info | Device has been turned off administratively. | `Device {1}, has been turned off administratively.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.psastor.device.state.permanentloss | Error | Device has been removed or is permanently inaccessible. | `Device {1} has been removed or is permanently inaccessible. Affected datastores (if any): {2}.` | <ul><li>{1}: Device ID</li><li>{2}: Value</li></ul> |
| esx.problem.psastor.device.state.permanentloss.noopens | Info | Permanently inaccessible device has no more opens. | `Permanently inaccessible device {1} has no more opens. It is now safe to unmount datastores (if any) {2} and delete the device.` | <ul><li>{1}: Device ID</li><li>{2}: Value</li></ul> |
| esx.problem.psastor.device.state.permanentloss.pluggedback | Error | Device has been plugged back in after being marked permanently inaccessible. | `Device {1} has been plugged back in after being marked permanently inaccessible. No data consistency guarantees.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.psastor.device.state.permanentloss.withreservationheld | Error | Device has been removed or is permanently inaccessible. | `Device {1} has been removed or is permanently inaccessible, while holding a reservation. Affected datastores (if any): {2}.` | <ul><li>{1}: Device ID</li><li>{2}: Value</li></ul> |
| esx.problem.psastor.device.too.many.io.error | Warning | Too many errors observed for device | `Too many errors observed for device {1} errPercentage {2}` | <ul><li>{1}: Device ID</li><li>{2}: Value</li></ul> |
| esx.problem.psastor.psastorpath.limitreached | Error | Maximum number of storage paths | `The maximum number of supported paths of {1} has been reached. Path {2} could not be added.` | <ul><li>{1}: Value</li><li>{2}: Path</li></ul> |
| esx.problem.psastor.unsupported.plugin.type | Warning | Storage plugin of unsupported type tried to register. | `Storage Device Allocation not supported for plugin type {1}` | <ul><li>{1}: Type</li></ul> |
| esx.problem.resourcegroup.delete.failed | Warning | Failed to delete resource group. | `Failed to delete resource groups with names '{rgnames}'.` | <ul><li>{rgnames}: Name</li></ul> |
| esx.problem.sched.latency.abort | Warning | Failed to Set the Virtual Machine's Latency Sensitivity | `Unable to apply latency-sensitivity setting to virtual machine {1}. No valid placement on the host.` | <ul><li>{1}: VM name</li></ul> |
| esx.problem.sched.qos.cat.noresource | Warning | No Cache Allocation Resource | `Unable to support cache allocation for virtual machine {1}. Out of resources.` | <ul><li>{1}: VM name</li></ul> |
| esx.problem.sched.qos.cat.notsupported | Warning | No Cache Allocation Support | `Unable to support L3 cache allocation for virtual machine {1}. No processor capabilities.` | <ul><li>{1}: VM name</li></ul> |
| esx.problem.sched.qos.cmt.noresource | Warning | No Cache Monitoring Resource | `Unable to support cache monitoring for virtual machine {1}. Out of resources.` | <ul><li>{1}: VM name</li></ul> |
| esx.problem.sched.qos.cmt.notsupported | Warning | No Cache Monitoring Support | `Unable to support L3 cache monitoring for virtual machine {1}. No processor capabilities.` | <ul><li>{1}: VM name</li></ul> |
| esx.problem.scratch.on.usb | Warning | Scratch is configured to SD-Card/USB device. This may result in system failure. Please add a secondary persistent device. | `Scratch is configured to SD-Card/USB device. This may result in system failure. Please add a secondary persistent device.` |  |
| esx.problem.scratch.partition.full | Warning | Scratch is low on available space ({1} MiB free). This may result in system failure. Please refer to KB article: KB 87212 | `Scratch is low on available space ({1} MiB free). This may result in system failure. Please refer to KB article: KB 87212` | <ul><li>{1}: Value</li></ul> |
| esx.problem.scratch.partition.size.small | Warning | Size of scratch partition is too small. | `Size of scratch partition {1} is too small. Recommended scratch partition size is {2} MiB.` | <ul><li>{1}: Partition</li><li>{2}: Value</li></ul> |
| esx.problem.scratch.partition.unconfigured | Warning | No scratch partition has been configured. | `No scratch partition has been configured. Recommended scratch partition size is {} MiB.` |  |
| esx.problem.scsi.apd.event.descriptor.alloc.failed | Warning | No memory to allocate APD Event | `No memory to allocate APD (All Paths Down) event subsystem.` |  |
| esx.problem.scsi.device.close.failed | Warning | Scsi Device close failed. | `"Failed to close the device {1} properly, plugin {2}.` | <ul><li>{1}: Device ID</li><li>{2}: Plugin</li></ul> |
| esx.problem.scsi.device.detach.failed | Warning | Device detach failed | `Detach failed for device :{1}. Exceeded the number of devices that can be detached, please cleanup stale detach entries.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.scsi.device.filter.attach.failed | Warning | Failed to attach filter to device. | `Failed to attach filters to device '%s' during registration. Plugin load failed or the filter rules are incorrect.` |  |
| esx.problem.scsi.device.invalid.xcopy.request | Warning | Invalid XCOPY request for device | `Invalid XCOPY request for device {1}. Host {2}, Device {3}, Plugin {4}, {5} sense, sense.key = {6}, sense.asc = {7}, sense.ascq = {8}: {9}` | <ul><li>{1}: Device ID</li><li>{2}: Host name</li><li>{3}: Device ID</li><li>{4}: Plugin</li><li>{5}: Value</li><li>{6}: Value</li><li>{7}: Value</li><li>{8}: Value</li><li>{9}: Value</li></ul> |
| esx.problem.scsi.device.io.bad.plugin.type | Warning | Plugin trying to issue command to device does not have a valid storage plugin type. | `Bad plugin type for device {1}, plugin {2}` | <ul><li>{1}: Device ID</li><li>{2}: Plugin</li></ul> |
| esx.problem.scsi.device.io.inquiry.failed | Warning | Failed to obtain INQUIRY data from the device | `Failed to get standard inquiry for device {1} from Plugin {2}.` | <ul><li>{1}: Device ID</li><li>{2}: Plugin</li></ul> |
| esx.problem.scsi.device.io.invalid.disk.qfull.value | Warning | Scsi device queue parameters incorrectly set. | `QFullSampleSize should be bigger than QFullThreshold. LUN queue depth throttling algorithm will not function as expected. Please set the QFullSampleSize and QFullThreshold disk configuration values in ESX correctly.` |  |
| esx.problem.scsi.device.io.latency.high | Warning | Scsi Device I/O Latency going high | `Device {1} performance has deteriorated. I/O latency increased from average value of {2} microseconds to {3} microseconds.` | <ul><li>{1}: Device ID</li><li>{2}: Microseconds</li><li>{3}: Microseconds</li></ul> |
| esx.problem.scsi.device.io.qerr.change.config | Warning | QErr cannot be changed on device. Please change it manually on the device if possible. | `QErr set to 0x{1} for device {2}. This may cause unexpected behavior. The system is not configured to change the QErr setting of device. The QErr value supported by system is 0x{3}. Please check the SCSI ChangeQErrSetting configuration value for ESX.` | <ul><li>{1}: Value</li><li>{2}: Device ID</li><li>{3}: Value</li></ul> |
| esx.problem.scsi.device.io.qerr.changed | Warning | Scsi Device QErr setting changed | `QErr set to 0x{1} for device {2}. This may cause unexpected behavior. The device was originally configured to the supported  QErr setting of 0x{3}, but this has been changed and could not be changed back.` | <ul><li>{1}: Value</li><li>{2}: Device ID</li><li>{3}: Value</li></ul> |
| esx.problem.scsi.device.is.local.failed | Warning | Plugin's isLocal entry point failed | `Failed to verify if the device {1} from plugin {2} is a local - not shared - device` | <ul><li>{1}: Device ID</li><li>{2}: Plugin</li></ul> |
| esx.problem.scsi.device.is.pseudo.failed | Warning | Plugin's isPseudo entry point failed | `Failed to verify if the device {1} from plugin {2} is a pseudo device` | <ul><li>{1}: Device ID</li><li>{2}: Plugin</li></ul> |
| esx.problem.scsi.device.is.ssd.failed | Warning | Plugin's isSSD entry point failed | `Failed to verify if the device {1} from plugin {2} is a Solid State Disk device` | <ul><li>{1}: Device ID</li><li>{2}: Plugin</li></ul> |
| esx.problem.scsi.device.limitreached | Error | Maximum number of storage devices | `The maximum number of supported devices of {1} has been reached. A device from plugin {2} could not be created.` | <ul><li>{1}: Value</li><li>{2}: Plugin</li></ul> |
| esx.problem.scsi.device.nmp.satp.option.failed | Warning | Failed to apply NMP SATP option during device discovery. | `Invalid config parameter: \"{1}\" provided in the nmp satp claimrule, this setting was not applied while claiming the path {2}` | <ul><li>{1}: Value</li><li>{2}: Path</li></ul> |
| esx.problem.scsi.device.state.off | Info | Device has been turned off administratively. | `Device {1}, has been turned off administratively.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.scsi.device.state.permanentloss | Error | Device has been removed or is permanently inaccessible. | `Device {1} has been removed or is permanently inaccessible. Affected datastores (if any): {2}.` | <ul><li>{1}: Device ID</li><li>{2}: Value</li></ul> |
| esx.problem.scsi.device.state.permanentloss.noopens | Info | Permanently inaccessible device has no more opens. | `Permanently inaccessible device {1} has no more opens. It is now safe to unmount datastores (if any) {2} and delete the device.` | <ul><li>{1}: Device ID</li><li>{2}: Value</li></ul> |
| esx.problem.scsi.device.state.permanentloss.pluggedback | Error | Device has been plugged back in after being marked permanently inaccessible. | `Device {1} has been plugged back in after being marked permanently inaccessible. No data consistency guarantees.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.scsi.device.state.permanentloss.withreservationheld | Error | Device has been removed or is permanently inaccessible. | `Device {1} has been removed or is permanently inaccessible, while holding a reservation. Affected datastores (if any): {2}.` | <ul><li>{1}: Device ID</li><li>{2}: Value</li></ul> |
| esx.problem.scsi.device.thinprov.atquota | Warning | Thin Provisioned Device Nearing Capacity | `Space utilization on thin-provisioned device {1} exceeded configured threshold. Affected datastores (if any): {2}.` | <ul><li>{1}: Device ID</li><li>{2}: Value</li></ul> |
| esx.problem.scsi.device.too.many.io.error | Warning | Too many errors observed for device | `Too many errors observed for device {1} errPercentage {2}` | <ul><li>{1}: Device ID</li><li>{2}: Value</li></ul> |
| esx.problem.scsi.scsipath.badpath.unreachpe | Error | vVol PE path going out of vVol-incapable adapter | `Sanity check failed for path {1}. The path is to a vVol PE, but it goes out of adapter {2} which is not PE capable. Path dropped.` | <ul><li>{1}: Path</li><li>{2}: Network adapter</li></ul> |
| esx.problem.scsi.scsipath.badpath.unsafepe | Error | Cannot safely determine vVol PE | `Sanity check failed for path {1}. Could not safely determine if the path is to a vVol PE. Path dropped.` | <ul><li>{1}: Path</li></ul> |
| esx.problem.scsi.scsipath.badpath.unsupported | Error | Discovery of PowerFlex SDC is not supported by ESX. | `Discovery of PowerFlex SDC is not supported by ESX, blocking path {1}.` | <ul><li>{1}: Path</li></ul> |
| esx.problem.scsi.scsipath.limitreached | Error | Maximum number of storage paths | `The maximum number of supported paths of {1} has been reached. Path {2} could not be added.` | <ul><li>{1}: Value</li><li>{2}: Path</li></ul> |
| esx.problem.scsi.unsupported.plugin.type | Warning | Storage plugin of unsupported type tried to register. | `Scsi Device Allocation not supported for plugin type {1}` | <ul><li>{1}: Type</li></ul> |
| esx.problem.sgx.addpackage | Info | Support for Intel Software Guard Extensions (SGX) has been disabled because a new CPU package was added to the host. Please refer to VMware Knowledge Base article 71367 for more details and remediation steps. | `Support for Intel Software Guard Extensions (SGX) has been disabled because a new CPU package was added to the host. Please refer to VMware Knowledge Base article 71367 for more details and remediation steps.` |  |
| esx.problem.sgx.htenabled | Info | Support for Intel Software Guard Extensions (SGX) has been disabled because HyperThreading is used by the host. Please refer to VMware Knowledge Base article 71367 for more details. | `Support for Intel Software Guard Extensions (SGX) has been disabled because HyperThreading is used by the host. Please refer to VMware Knowledge Base article 71367 for more details.` |  |
| esx.problem.slp.deprecated | Warning | CIM service on ESXi is deprecated and will be removed in the next major release. SLP is enabled on the host. Please refer to KB 95798 for more details. | `CIM service on ESXi is deprecated and will be removed in the next major release. SLP is enabled on the host. Please refer to KB 95798 for more details.` |  |
| esx.problem.storage.apd.start | Warning | All paths are down | `Device or filesystem with identifier {1} has entered the All Paths Down state.` | <ul><li>{1}: Identifier</li></ul> |
| esx.problem.storage.apd.timeout | Warning | All Paths Down timed out, I/Os will be fast failed | `Device or filesystem with identifier {1} has entered the All Paths Down Timeout state after being in the All Paths Down state for {2} seconds. I/Os will now be fast failed.` | <ul><li>{1}: Identifier</li><li>{2}: Seconds</li></ul> |
| esx.problem.storage.connectivity.devicepor | Warning | Frequent PowerOn Reset Unit Attention of Storage Path | `Frequent PowerOn Reset Unit Attentions are occurring on device {1}. This might indicate a storage problem. Affected datastores: {2}` | <ul><li>{1}: Device ID</li><li>{2}: Datastore(s)</li></ul> |
| esx.problem.storage.connectivity.lost | Error | Lost Storage Connectivity | `Lost connectivity to storage device {1}. Path {2} is down. Affected datastores: {3}.` | <ul><li>{1}: Device ID</li><li>{2}: Path</li><li>{3}: Datastore(s)</li></ul> |
| esx.problem.storage.connectivity.pathpor | Warning | Frequent PowerOn Reset Unit Attention of Storage Path | `Frequent PowerOn Reset Unit Attentions are occurring on path {1}. This might indicate a storage problem. Affected device: {2}.  Affected datastores: {3}` | <ul><li>{1}: Path</li><li>{2}: Value</li><li>{3}: Datastore(s)</li></ul> |
| esx.problem.storage.connectivity.pathstatechanges | Info | Frequent State Changes of Storage Path | `Frequent path state changes are occurring for path {1}. This might indicate a storage problem. Affected device: {2}. Affected datastores: {3}` | <ul><li>{1}: Path</li><li>{2}: Value</li><li>{3}: Datastore(s)</li></ul> |
| esx.problem.storage.iscsi.connection.error.cmd.sn.max | Error | iSCSI connection command window closed error. | `iSCSI connection {1} on device {2} is being reset because command window is closed.` | <ul><li>{1}: Value</li><li>{2}: Device ID</li></ul> |
| esx.problem.storage.iscsi.discovery.connect.error | Error | iSCSI discovery target login connection problem | `iSCSI discovery to {1} on {2} failed. The iSCSI Initiator could not establish a network connection to the discovery address.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.storage.iscsi.discovery.login.error | Error | iSCSI Discovery target login error | `iSCSI discovery to {1} on {2} failed. The Discovery target returned a login error of: {3}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.storage.iscsi.isns.discovery.error | Error | iSCSI iSns Discovery error | `iSCSI iSns discovery to {1} on {2} failed. ({3} : {4}).` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.problem.storage.iscsi.target.connect.error | Error | iSCSI Target login connection problem | `Login to iSCSI target {1} on {2} failed. The iSCSI initiator could not establish a network connection to the target.` | <ul><li>{1}: Target</li><li>{2}: Value</li></ul> |
| esx.problem.storage.iscsi.target.login.error | Error | iSCSI Target login error | `Login to iSCSI target {1} on {2} failed. Target returned login error of: {3}.` | <ul><li>{1}: Target</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.storage.iscsi.target.permanently.lost | Error | iSCSI target permanently removed | `The iSCSI target {2} was permanently removed from {1}.` | <ul><li>{1}: Value</li><li>{2}: Target</li></ul> |
| esx.problem.storage.iscsi.target.permanently.removed | Error | iSCSI target was permanently removed | `The iSCSI target {1} was permanently removed from {2}.` | <ul><li>{1}: Target</li><li>{2}: Value</li></ul> |
| esx.problem.storage.redundancy.degraded | Warning | Degraded Storage Path Redundancy | `Path redundancy to storage device {1} degraded. Path {2} is down. Affected datastores: {3}.` | <ul><li>{1}: Device ID</li><li>{2}: Path</li><li>{3}: Datastore(s)</li></ul> |
| esx.problem.storage.redundancy.lost | Warning | Lost Storage Path Redundancy | `Lost path redundancy to storage device {1}. Path {2} is down. Affected datastores: {3}.` | <ul><li>{1}: Device ID</li><li>{2}: Path</li><li>{3}: Datastore(s)</li></ul> |
| esx.problem.swap.systemSwap.isPDL.cannot.remove | Warning | System swap at path {1} was affected by the PDL of its datastore and was removed. System swap has been reconfigured. | `System swap at path {1} was affected by the PDL of its datastore and was removed. System swap has been reconfigured.` | <ul><li>{1}: Path</li></ul> |
| esx.problem.swap.systemSwap.isPDL.cannot.remove.2 | Warning | System swap was affected by the PDL of its datastore and was removed. System swap has been reconfigured. | `System swap was affected by the PDL of {1} and was removed. System swap has been reconfigured.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.swap.systemSwap.isPDL.removed.reconfig.failure | Warning | System swap at path {1} was affected by the PDL of its datastore. It was removed but the subsequent reconfiguration failed. | `System swap at path {1} was affected by the PDL of its datastore. It was removed but the subsequent reconfiguration failed.` | <ul><li>{1}: Path</li></ul> |
| esx.problem.swap.systemSwap.isPDL.removed.reconfig.failure.2 | Warning | System swap was affected by the PDL of its datastore. It was removed but the subsequent reconfiguration failed. | `System swap was affected by the PDL of {1}. It was removed but the subsequent reconfiguration failed.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.syslog.config | Warning | System logging is not configured. | `System logging is not configured on host {host.name}. Please check Syslog options for the host under Configuration -> Software -> Advanced Settings in vSphere client.` | <ul><li>{host.name}: Host name</li></ul> |
| esx.problem.syslog.nonpersistent | Warning | System logs are stored on non-persistent storage. | `System logs on host {host.name} are stored on non-persistent storage. Consult product documentation to configure a syslog server or a scratch partition.` | <ul><li>{host.name}: Host name</li></ul> |
| esx.problem.test.test0 | Error | Test with no arguments | `Test with no arguments` |  |
| esx.problem.test.test2 | Error | Test with both int and string arguments | `Test with both {1} and {2}` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.unsupported.tls.protocols | Warning | Upgrade detected unsupported TLS protocols, resetting option /UserVars/ESXiVPsDisabledProtocols to default. From vSphere 8.0 onwards, protocols prior to tlsv1.2 are no longer supported and must remain disabled. | `Upgrade detected unsupported TLS protocols, resetting option /UserVars/ESXiVPsDisabledProtocols to default. From vSphere 8.0 onwards, protocols prior to tlsv1.2 are no longer supported and must remain disabled.` |  |
| esx.problem.vfat.filesystem.full.other | Error | A VFAT filesystem is full. | `The VFAT filesystem {1} (UUID {2}) is full.` | <ul><li>{1}: Value</li><li>{2}: ID</li></ul> |
| esx.problem.vfat.filesystem.full.scratch | Error | A VFAT filesystem, being used as the host's scratch partition, is full. | `The host's scratch partition, which is the VFAT filesystem {1} (UUID {2}), is full.` | <ul><li>{1}: Value</li><li>{2}: ID</li></ul> |
| esx.problem.visorfs.configstore.usage.error | Error | Configstore is reaching its critical size limit. Please refer to the KB 93362 for more details. | `Ramdisk '{1}' is reaching its critical size limit. Approx {2}% space left. Please refer to the KB 93362 for more details.` | <ul><li>{1}: Ramdisk name</li><li>{2}: Percentage</li></ul> |
| esx.problem.visorfs.configstore.usage.warning | Warning | A ramdisk has a very high usage. Please refer to the KB 93362 for more details. | `Ramdisk '{1}' usage is very high. Approx {2}% space left. Please refer to the KB 93362 for more details.` | <ul><li>{1}: Ramdisk name</li><li>{2}: Percentage</li></ul> |
| esx.problem.visorfs.failure | Error | An operation on the root filesystem has failed. | `An operation on the root filesystem has failed.` |  |
| esx.problem.visorfs.inodetable.full | Error | The root filesystem's file table is full. | `The root filesystem's file table is full.  As a result, the file {1} could not be created by the application '{2}'.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.visorfs.ramdisk.full | Error | A ramdisk is full. | `The ramdisk '{1}' is full.  As a result, the file {2} could not be written.` | <ul><li>{1}: Ramdisk name</li><li>{2}: Value</li></ul> |
| esx.problem.visorfs.ramdisk.inodetable.full | Error | A ramdisk's file table is full. | `The file table of the ramdisk '{1}' is full.  As a result, the file {2} could not be created by the application '{3}'.` | <ul><li>{1}: Ramdisk name</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.visorfs.ramdisk.usage.error | Error | Config store is reaching its critical size limit. | `Ramdisk '{1}' is reaching its critical size limit. Approx {2}% space left.` | <ul><li>{1}: Ramdisk name</li><li>{2}: Percentage</li></ul> |
| esx.problem.visorfs.ramdisk.usage.warning | Warning | A ramdisk has a very high usage. | `Ramdisk '{1}' usage is very high. Approx {2}% space left.` | <ul><li>{1}: Ramdisk name</li><li>{2}: Percentage</li></ul> |
| esx.problem.vm.kill.unexpected.fault.failure | Error | A VM could not fault in the a page. The VM is terminated as further progress is impossible. | `The VM using the config file {1} could not fault in a guest physical page from the hypervisor level swap file at {2}. The VM is terminated as further progress is impossible.` | <ul><li>{1}: Value</li><li>{2}: File path</li></ul> |
| esx.problem.vm.kill.unexpected.fault.failure.2 | Error | A virtual machine could not fault in the a page. It is terminated as further progress is impossible. | `{1} could not fault in a guest physical page from the hypervisor level swap file on {2}. The VM is terminated as further progress is impossible` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.vm.kill.unexpected.forcefulPageRetire | Error | A VM did not respond to swap actions and is forcefully powered off to prevent system instability. | `The VM using the config file {1} contains the host physical page {2} which was scheduled for immediate retirement. To avoid system instability the VM is forcefully powered off.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.vm.kill.unexpected.forcefulPageRetire.64 | Error | A VM did not respond to swap actions and is forcefully powered off to prevent system instability. | `The VM using the config file {1} contains the host physical page {2} which was scheduled for immediate retirement. To avoid system instability the VM is forcefully powered off.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.vm.kill.unexpected.forcefulPageRetire.64.2 | Error | A virtual machine cointained a host physical page that was scheduled for immediate retirement.  To avoid system instability the virtual machine is forcefully powered off. | `{1} contained the host physical page {2} which was scheduled for immediate retirement. To avoid system instability the virtual machine is forcefully powered off.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.vm.kill.unexpected.noSwapResponse | Error | A VM did not respond to swap actions and is forcefully powered off to prevent system instability. | `The VM using the config file {1} did not respond to {2} swap actions in {3} seconds and is forcefully powered off to prevent system instability.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Seconds</li></ul> |
| esx.problem.vm.kill.unexpected.noSwapResponse.2 | Error | A virtual machine did not respond to swap actions. It is terminated as further progress is impossible. | `{1} did not respond to {2} swap actions in {3} seconds and is forcefully powered off to prevent system instability.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Seconds</li></ul> |
| esx.problem.vm.kill.unexpected.vmtrack | Error | A VM is allocating too many pages while system is critically low in free memory. It is forcefully terminated to prevent system instability. | `The VM using the config file {1} is allocating too many pages while system is critically low in free memory. It is forcefully terminated to prevent system instability.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vm.kill.unexpected.vmtrack.2 | Error | A virtual machine is allocating too many pages while system is critically low in free memory. It is forcefully terminated to prevent system instability. | `{1} is allocating too many pages while system is critically low in free memory. It is forcefully terminated to prevent system instability.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vm.kill.unexpected.vmx.fault.failure.2 | Error | A user world daemon of a virtual machine could not fault in the a page. The VM is terminated as further progress is impossible. | `The user world daemon of {1} could not fault in a page. The virtual machine is terminated as further progress is impossible.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vmfs.ats.incompatibility.detected | Error | Multi-extent ATS-only VMFS Volume unable to use ATS | `Multi-extent ATS-only volume '{1}' ({2}) is unable to use ATS because HardwareAcceleratedLocking is disabled on this host: potential for introducing filesystem corruption. Volume should not be used from other hosts.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.vmfs.ats.support.lost | Error | Device Backing VMFS has lost ATS Support | `ATS-Only VMFS volume '{1}' not mounted. Host does not support ATS or ATS initialization has failed.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vmfs.error.volume.is.locked | Error | VMFS Locked By Remote Host | `Volume on device {1} is locked, possibly because some remote host encountered an error during a volume operation and could not recover.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vmfs.extent.offline | Error | Device backing an extent of a file system is offline. | `An attached device {1} may be offline. The file system {2} is now in a degraded state. While the datastore is still available, parts of data that reside on the extent that went offline might be inaccessible.` | <ul><li>{1}: Device ID</li><li>{2}: File system</li></ul> |
| esx.problem.vmfs.extent.online | Info | Device backing an extent of a file system came online | `Device {1} backing file system {2} came online. This extent was previously offline. All resources on this device are now available.` | <ul><li>{1}: Device ID</li><li>{2}: File system</li></ul> |
| esx.problem.vmfs.heartbeat.corruptondisk | Error | VMFS Heartbeat Corruption Detected. | `At least one corrupt on-disk heartbeat region was detected on volume {1} ({2}). Other regions of the volume might be damaged too.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| esx.problem.vmfs.heartbeat.recovered | Info | VMFS Volume Connectivity Restored | `Successfully restored access to volume {1} ({2}) following connectivity issues.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| esx.problem.vmfs.heartbeat.timedout | Info | VMFS Volume Connectivity Degraded | `Lost access to volume {1} ({2}) due to connectivity issues. Recovery attempt is in progress and outcome will be reported shortly.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| esx.problem.vmfs.heartbeat.unrecoverable | Error | VMFS Volume Connectivity Lost | `Lost connectivity to volume {1} ({2}) and subsequent recovery attempts have failed.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| esx.problem.vmfs.journal.createfailed | Error | No Space To Create VMFS Journal | `No space for journal on volume {1} ({2}). Volume will remain in read-only metadata mode with limited write support until journal can be created.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| esx.problem.vmfs.lock.busy.filedesc | Error | Trying to acquire lock on an already locked file. - File description | `{1} Lock(s) held on a file on volume {2}. numHolders: {3}. gblNumHolders: {4}. Locking Host(s) MAC: {5}` | <ul><li>{1}: Value</li><li>{2}: Volume</li><li>{3}: Value</li><li>{4}: Value</li><li>{5}: Value</li></ul> |
| esx.problem.vmfs.lock.busy.filename | Error | Trying to acquire lock on an already locked file. Filename | `Lock(s) held on file {1} by other host(s).` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vmfs.lock.corruptondisk | Error | VMFS Lock Corruption Detected | `At least one corrupt on-disk lock was detected on volume {1} ({2}). Other regions of the volume might be damaged too.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| esx.problem.vmfs.lock.corruptondisk.v2 | Error | VMFS Lock Corruption Detected | `At least one corrupt on-disk lock was detected on volume {1} ({2}). Other regions of the volume might be damaged too.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| esx.problem.vmfs.lockmode.inconsistency.detected | Error | Inconsistent VMFS lockmode detected. | `Inconsistent lockmode change detected for VMFS volume '{1} ({2})': volume was configured for {3} lockmode at time of open and now it is configured for {4} lockmode but this host is not using {5} lockmode. Protocol error during ATS transition. Volume descriptor refresh operations will fail until this host unmounts and remounts the volume.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li><li>{5}: Value</li></ul> |
| esx.problem.vmfs.nfs.mount.connect.failed | Error | Unable to connect to NFS server | `Failed to mount to the server {1} mount point {2}. {3}` | <ul><li>{1}: Server</li><li>{2}: Mount point</li><li>{3}: Value</li></ul> |
| esx.problem.vmfs.nfs.mount.failed | Error | Failed to mount NFS volume | `NFS mount failed for {1}:{2} volume {3}. Status: {4}` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Volume</li><li>{4}: Status</li></ul> |
| esx.problem.vmfs.nfs.mount.limit.exceeded | Error | NFS has reached the maximum number of supported volumes | `Failed to mount to the server {1} mount point {2}. {3}` | <ul><li>{1}: Server</li><li>{2}: Mount point</li><li>{3}: Value</li></ul> |
| esx.problem.vmfs.nfs.server.disconnect | Error | Lost connection to NFS server | `Lost connection to server {1} mount point {2} mounted as {3} ({4}).` | <ul><li>{1}: Server</li><li>{2}: Mount point</li><li>{3}: Mount name</li><li>{4}: Value</li></ul> |
| esx.problem.vmfs.nfs.vmknic.removed | Warning | vmknic configured for NFS has been removed | `vmknic {1} removed, NFS{2} datastore {3} configured with the vmknic will be inaccessible.` | <ul><li>{1}: Physical NIC</li><li>{2}: Value</li><li>{3}: Datastore(s)</li></ul> |
| esx.problem.vmfs.nfs.volume.io.latency.exceed.threshold.period | Warning | NFS volume average I/O Latency has exceeded configured threshold for the current configured period | `NFS volume {1} average I/O latency {2}(us) has exceeded threshold {3}(us) for last {4} minutes` | <ul><li>{1}: Volume</li><li>{2}: Microseconds</li><li>{3}: Microseconds</li><li>{4}: Duration (minutes)</li></ul> |
| esx.problem.vmfs.nfs.volume.io.latency.high | Warning | NFS volume I/O Latency going high | `NFS volume {1} performance has deteriorated. I/O latency increased from average value of {2} microseconds to {3} microseconds.` | <ul><li>{1}: Volume</li><li>{2}: Microseconds</li><li>{3}: Microseconds</li></ul> |
| esx.problem.vmfs.nfs.volume.io.latency.high.exceed.threshold | Warning | NFS volume I/O Latency exceeding threshold | `NFS volume {1} performance has deteriorated. I/O latency increased from average value of {2} microseconds to {3} microseconds. Exceeded threshold {4} microseconds` | <ul><li>{1}: Volume</li><li>{2}: Microseconds</li><li>{3}: Microseconds</li><li>{4}: Microseconds</li></ul> |
| esx.problem.vmfs.nfs.volume.no.space | Warning | No space on NFS volume. | `{1}: No space on NFS volume.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vmfs.resource.corruptondisk | Error | VMFS Resource Corruption Detected | `At least one corrupt resource metadata region was detected on volume {1} ({2}). Other regions of the volume might be damaged too.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| esx.problem.vmfs.spanned.lockmode.inconsistency.detected | Error | Inconsistent VMFS lockmode detected on spanned volume. | `Inconsistent lockmode change detected for spanned VMFS volume '{1} ({2})': volume was configured for {3} lockmode at time of open and now it is configured for {4} lockmode but this host is not using {5} lockmode. All operations on this volume will fail until this host unmounts and remounts the volume.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li><li>{5}: Value</li></ul> |
| esx.problem.vmfs.spanstate.incompatibility.detected | Error | Incompatible VMFS span state detected. | `Incompatible span change detected for VMFS volume '{1} ({2})': volume was not spanned at time of open but now it is, and this host is using ATS-only lockmode but the volume is not ATS-only. Volume descriptor refresh operations will fail until this host unmounts and remounts the volume.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.vmsyslogd.auditrecord.local.disabled | Error | The local storage of audit records is disabled | `The local storage of audit records is disabled due to the error: {1}. Please refer to KB-386607 for remediation steps.` | <ul><li>{1}: Error message</li></ul> |
| esx.problem.vmsyslogd.remote.auditrecords.dropped | Error | Audit recording to remote host has dropped. | `The host "{1}://{2}:{3}" has lost {4} audit records.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.problem.vmsyslogd.remote.failure | Error | Remote logging host has become unreachable. | `The host "{1}://{2}:{3}" has become unreachable.  Remote logging to this host has stopped.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.vmsyslogd.remote.logmessages.dropped | Error | Logging to remote host has dropped. | `The host "{1}://{2}:{3}" has lost {4} log messages.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.problem.vmsyslogd.storage.failure | Error | Logging to storage has failed. | `Logging to storage has failed.  Logs are no longer being stored locally on this host.` |  |
| esx.problem.vmsyslogd.storage.logdir.invalid | Error | The configured log directory cannot be used.  The default directory will be used instead. | `The configured log directory {1} cannot be used.  The default directory {2} will be used instead.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.vmsyslogd.unexpected | Error | Log daemon has failed for an unexpected reason. | `Log daemon has failed for an unexpected reason: {1}` | <ul><li>{1}: Reason</li></ul> |
| esx.problem.vob.vsan.dom.errorfixed | Warning | vSAN detected and fixed a medium or checksum error. | `vSAN detected and fixed a medium or checksum error for component {1} on disk group {2}.` | <ul><li>{1}: Value</li><li>{2}: Group</li></ul> |
| esx.problem.vob.vsan.dom.lsnmismatcherror | Warning | vSAN detected LSN mismatch in mirrors | `vSAN detected LSN mismatch in mirrors for object {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vob.vsan.dom.nospaceduringresync | Warning | Resync encountered no space error | `Resync encountered no space error for component {1} on disk {2}. Resync will resume once space is freed up on this disk. Need around {3}MB to resync the component on this disk` | <ul><li>{1}: Value</li><li>{2}: Disk</li><li>{3}: Value</li></ul> |
| esx.problem.vob.vsan.dom.resyncdecisiondelayed | Warning | Resync is delayed. | `Resync is delayed for component {1} on disk {2} until data availability is regained for object {3} on the remote site.` | <ul><li>{1}: Value</li><li>{2}: Disk</li><li>{3}: Value</li></ul> |
| esx.problem.vob.vsan.dom.resynctimeout | Warning | Resync timed out | `Resync timed out as no progress was made in {1} minute(s) for component {2} on disk {3}. Resync will be tried again for this component. The remaining resync is around {4}MB.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Disk</li><li>{4}: Value</li></ul> |
| esx.problem.vob.vsan.dom.singlediskerrorfixed | Warning | vSAN detected and fixed a medium or checksum error. | `vSAN detected and fixed a medium or checksum error for component {1} on disk {2}.` | <ul><li>{1}: Value</li><li>{2}: Disk</li></ul> |
| esx.problem.vob.vsan.dom.singlediskunrecoverableerror | Warning | vSAN detected an unrecoverable medium or checksum error. | `vSAN detected an unrecoverable medium or checksum error for component {1} on disk {2}.` | <ul><li>{1}: Value</li><li>{2}: Disk</li></ul> |
| esx.problem.vob.vsan.dom.unrecoverableerror | Warning | vSAN detected an unrecoverable medium or checksum error. | `vSAN detected an unrecoverable medium or checksum error for component {1} on disk group {2}.` | <ul><li>{1}: Value</li><li>{2}: Group</li></ul> |
| esx.problem.vob.vsan.lsom.backupfailednvmediskhealthcriticalwarning | Error | NVMe critical health warning for disk.  The disk's backup device has failed. | `NVMe critical health warning for disk {1}.  The disk's backup device has failed.` | <ul><li>{1}: Disk</li></ul> |
| esx.problem.vob.vsan.lsom.componentoffline | Warning | Offline event on component. | `Offline event issued for component: {1}, flag: {2}, reason: {3}.` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Reason</li></ul> |
| esx.problem.vob.vsan.lsom.componentthreshold | Warning | vSAN Node: Near node component count limit. | `vSAN Node: {1} reached threshold of {2} %% opened components ({3} of {4}).` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.problem.vob.vsan.lsom.ddhEvacFailed | Error | Evacuation has failed for device and it will be retried by DDH. | `Evacuation has failed for device {1} and it will be retried by DDH.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.devicerepair | Error | vSAN device is being repaired due to I/O failures. | `vSAN device {1} is being repaired due to I/O failures, and will be out of service until the repair is complete. If the device is part of a dedup disk group, the entire disk group will be out of service until the repair is complete.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.devicewithhighlatency | Error | vSAN device has high latency. It will be evacuated and unmounted, consider replacing it. | `vSAN device {1} has high latency. It will be evacuated and unmounted, consider replacing it.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.devicewithsmartfailure | Error | vSAN device smart health status is impending failure. It will be evacuated and unmounted, consider replacing it. | `vSAN device {1} smart health status is impending failure. It will be evacuated and unmounted, consider replacing it.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.diskerror | Error | vSAN device is under permanent failure. | `vSAN device {1} is under permanent failure.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.diskgrouplimit | Error | Failed to create a new disk group. | `Failed to create new disk group {1}. The system has reached the maximum amount of disks groups allowed {2} for the current amount of memory {3}. Add more memory.` | <ul><li>{1}: Group</li><li>{2}: Value</li><li>{3}: Memory</li></ul> |
| esx.problem.vob.vsan.lsom.diskgrouplogcongested | Error | vSAN diskgroup log is congested. | `vSAN diskgroup {1} log is congested.` | <ul><li>{1}: Group</li></ul> |
| esx.problem.vob.vsan.lsom.diskgroupundercongestion | Warning | vSAN disk group is under congestion. It will be remediated. No action is needed. | `vSAN disk group {1} is under {2} congestion. It will be remediated. No action is needed.` | <ul><li>{1}: Group</li><li>{2}: Value</li></ul> |
| esx.problem.vob.vsan.lsom.disklimit2 | Error | Failed to add disk to disk group. | `Failed to add disk {1} to disk group. The system has reached the maximum amount of disks allowed {2} for the current amount of memory {3} GB. Add more memory.` | <ul><li>{1}: Disk</li><li>{2}: Value</li><li>{3}: Memory</li></ul> |
| esx.problem.vob.vsan.lsom.diskpropagatederror | Error | vSAN device is under propagated error. | `vSAN device {1} is under propagated error.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.diskpropagatedpermerror | Error | vSAN device is under propagated permanent error. | `vSAN device {1} is under propagated permanent error.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.diskunhealthy | Error | vSAN device is unhealthy. | `vSAN device {1} is unhealthy.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.evacFailedInsufficientResources | Error | Evacuation failed for device due to insufficient resources and it will be retried. | `Evacuation failed for device {1} due to insufficient resources and it will be retried. Please make resources available for evacuation.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.invalidMetadataComponent | Warning | Deleted invalid metadata component. | `Deleted invalid metadata component: {1}.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vob.vsan.lsom.metadataURE | Error | vSAN device is being evacuated and rebuilt due to an unrecoverable read error. | `vSAN device {1} encountered an unrecoverable read error. This disk will be evacuated and rebuilt. If the device is part of a dedup disk group, the entire disk group will be evacuated and rebuilt.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.readonlynvmediskhealthcriticalwarning | Error | NVMe disk critical health warning for disk.  Disk is now read only. | `NVMe critical health warning for disk {1} is: The NVMe disk has become read only.` | <ul><li>{1}: Disk</li></ul> |
| esx.problem.vob.vsan.lsom.reliabilitynvmediskhealthcriticalwarning | Error | NVMe critical health warning for disk.  The disk has become unreliable. | `NVMe critical health warning for disk {1}.  The disk has become unreliable.` | <ul><li>{1}: Disk</li></ul> |
| esx.problem.vob.vsan.lsom.sparecapacitynvmediskhealthcriticalwarning | Error | NVMe critical health warning for disk.  The disk's spare capacity is below threshold. | `NVMe critical health warning for disk {1}.  The disk's spare capacity is below threshold.` | <ul><li>{1}: Disk</li></ul> |
| esx.problem.vob.vsan.lsom.storagepoolURE | Error | vSAN device is being evacuated and rebuilt due to an unrecoverable read error. | `vSAN device {1} encountered an unrecoverable read error. This disk will be rebuilt.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.storagepoolrepair | Error | vSAN device is being repaired due to I/O failures. | `vSAN device {1} is being repaired due to I/O failures and will be out of service until the repair is complete.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.storagepoolstuckio | Error | No response for I/O on vSAN device. | `No response for I/O on vSAN device {1}.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.stuckio | Error | vSAN device detected suspended I/Os. | `vSAN device {1} detected suspended I/Os. Taking the host out of service to avoid affecting the vSAN cluster.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.stuckiooffline | Error | vSAN device detected stuck I/O error. | `vSAN device {1} detected stuck I/O error. Marking the device as offline.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.stuckiopropagated | Error | vSAN device is under propagated stuck I/O error. | `vSAN device {1} is under propagated stuck I/O error. Marking the device as offline.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.stuckiotimeout | Error | vSAN device detected I/O timeout error. | `vSAN device {1} detected I/O timeout error. This may lead to stuck I/O.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.lsom.temperaturenvmediskhealthcriticalwarning | Error | NVMe critical health warning for disk.  The disk's temperature is beyond threshold. | `NVMe critical health warning for disk {1}.  The disk's temperature beyond threshold.` | <ul><li>{1}: Disk</li></ul> |
| esx.problem.vob.vsan.pdl.offline | Error | vSAN device has gone offline. | `vSAN device {1} has gone offline.` | <ul><li>{1}: Device ID</li></ul> |
| esx.problem.vob.vsan.siteTakeover.takeoverTsMismatch | Warning | Objects recovered from the site takeover are found. A host reboot is required to access these objects. | `Objects recovered from the site takeover are found. A host reboot is required to access these objects.` |  |
| esx.problem.vob.vsan.zdom.failstoppaused | Warning | A ZDOM object is paused due to continuous fail-stops. | `ZDOM object {1} is paused on host {2}, numFailStops={3}.` | <ul><li>{1}: Value</li><li>{2}: Host name</li><li>{3}: Value</li></ul> |
| esx.problem.vobdtestcorrelator.test.0 | Info | Test with no arguments. | `Test with no arguments` |  |
| esx.problem.vobdtestcorrelator.test.1d | Info | Test with int argument. | `Test with int argument: {1}` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vobdtestcorrelator.test.1s | Info | Test with sting argument. | `Test with sting argument: {1}` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vobdtestcorrelator.test.hugestr | Info | Test with huge sting argument. | `Test with huge sting argument: {1}` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vpxa.core.dumped | Warning | Vpxa crashed and a core file was created. | `{1} crashed ({2} time(s) so far) and a core file might have been created at {3}. This might have caused connections to the host to be dropped.` | <ul><li>{1}: Value</li><li>{2}: Count</li><li>{3}: File path</li></ul> |
| esx.problem.vpxa.core.dumped.encrypted | Warning | Vpxa crashed and an encrypted core file was created. | `{1} crashed ({2} time(s) so far) and an encrypted core file using keyId {3} might have been created at {4}. This might have caused connections to the host to be dropped.` | <ul><li>{1}: Value</li><li>{2}: Count</li><li>{3}: ID</li><li>{4}: File path</li></ul> |
| esx.problem.vsan.clustering.disabled | Warning | vSAN clustering services have been disabled. | `vSAN clustering and directory services have been disabled thus will be no longer available.` |  |
| esx.problem.vsan.dom.component.datacomponent.on.witness.host | Warning | Data component found on witness host. | `Data component {1} found on witness host is ignored.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vsan.dom.init.failed.status | Warning | vSAN Distributed Object Manager failed to initialize | `vSAN Distributed Object Manager failed to initialize. While the ESXi host might still be part of the vSAN cluster, some of the vSAN related services might fail until this problem is resolved. Failure Status: {1}.` | <ul><li>{1}: Status</li></ul> |
| esx.problem.vsan.health.ssd.endurance | Info | One or more disks exceed its/their warning usage of estimated endurance threshold. | `Disks {Disk Name} in Cluster {Cluster Name} have exceeded warning usage of their estimated endurance threshold {Disk Percentage Threshold}, currently at {Disk Percentage Used} percent usage (respectively), based on SMART data. The percentage usage ranges from 0 to 255, inclusive. Instances where the usage exceeds 100 percent are uncommon.` | <ul><li>{Disk Name}: Name</li><li>{Cluster Name}: Cluster</li><li>{Disk Percentage Threshold}: Disk Percentage Threshold</li><li>{Disk Percentage Used}: Disk Percentage Used</li></ul> |
| esx.problem.vsan.health.ssd.endurance.error | Error | One of the disks exceeds the estimated endurance threshold. | `Disks {1} have exceeded their estimated endurance threshold, currently at {2} percent usage (respectively), based on SMART data. The percentage usage ranges from 0 to 255, inclusive. Instances where the usage exceeds 100 percent are uncommon.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.vsan.health.ssd.endurance.warning | Warning | One of the disks exceeds 90% of its estimated endurance threshold. | `Disks {1} have exceeded 90 percent usage of their estimated endurance threshold, currently at {2} percent usage (respectively), based on SMART data. The percentage usage ranges from 0 to 255, inclusive. Instances where the usage exceeds 100 percent are uncommon.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.vsan.health.vsanesa.pdl | Error | One of the disks is detected with PDL in vSAN ESA Cluster. Please check the host for further details. | `Disk {1} is detected with PDL in vSAN ESA Cluster. Please check the host for further details.` | <ul><li>{1}: Disk</li></ul> |
| esx.problem.vsan.lsom.congestionthreshold | Info | vSAN device Memory/SSD congestion has changed. | `LSOM {1} Congestion State: {2}. Congestion Threshold: {3} Current Congestion: {4}.` | <ul><li>{1}: Value</li><li>{2}: State</li><li>{3}: Value</li><li>{4}: Value</li></ul> |
| esx.problem.vsan.net.not.ready | Error | A vmknic added to vSAN network configuration doesn't have valid IP. Network is not ready. | `vmknic {1} that is currently configured to be used with vSAN doesn't have an IP address yet. There are no other active network configuration and therefore the vSAN node doesn't have network connectivity.` | <ul><li>{1}: Physical NIC</li></ul> |
| esx.problem.vsan.net.redundancy.lost | Warning | vSAN doesn't haven any redundancy in its network configuration. | `vSAN network configuration doesn't have any redundancy. This might be a problem if further network configuration is removed.` |  |
| esx.problem.vsan.net.redundancy.reduced | Warning | vSAN is operating on reduced network redundancy. | `vSAN network configuration redundancy has been reduced. This might be a problem if further network configuration is removed.` |  |
| esx.problem.vsan.no.network.connectivity | Error | vSAN doesn't have any network configuration for use. | `vSAN doesn't have any network configuration. This can severely impact several objects in the vSAN datastore.` |  |
| esx.problem.vsan.vmknic.not.ready | Warning | A vmknic added to vSAN network configuration doesn't have valid IP. It will not be in use. | `vmknic {1} that is currently configured to be used with vSAN doesn't have an IP address yet. However, there are other network configuration which are active. If those configurations are removed that may cause problems.` | <ul><li>{1}: Physical NIC</li></ul> |
| esx.problem.vscsi.npiv.configuration.dropped | Warning | NPIV configuration dropped for VDISK | `NPIV configuration dropped - VDISK scsi{1}:{2} (VMName: {3}).` | <ul><li>{1}: Value</li><li>{2}: Value</li><li>{3}: Value</li></ul> |
| esx.problem.vscsi.shared.vmdk.add.failure.max.count | Error |  Failed to add shared virtual disk. Maximum count reached | `Failed to add shared virtual disk. Maximum number of shared vmdks supported per ESX host are {1}` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vscsi.shared.vmdk.filter.attach.failed | Error | Filter attach failed | `Failed to attach filter {1} on disk {2} using physical bus sharing for world {3} as it is not supported.` | <ul><li>{1}: Value</li><li>{2}: Disk</li><li>{3}: World ID</li></ul> |
| esx.problem.vscsi.shared.vmdk.no.free.slot.available | Error | No free slots available | `No Free slot available. Maximum number of virtual machinies supported in MSCS cluster are {1}` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vscsi.shared.vmdk.virtual.machine.power.on.failed | Error | Failed to power on virtual machines on shared VMDK with running virtual machine | `Two or more virtual machines (\"{1}\" and \"{2}\") sharing same virtual disk are not allowed to be Powered-On on same host.` | <ul><li>{1}: Value</li><li>{2}: Value</li></ul> |
| esx.problem.vvol.container.offline | Error | VVol container has gone offline. | `VVol container {1} has gone offline.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vvol.container.vasaprovider.inaccessible | Info | The VVol container on the host has no working VASA Providers. | `The VVol container {1} on the host {2} has no working VASA Providers.` | <ul><li>{1}: Value</li><li>{2}: Host name</li></ul> |
| esx.problem.vvol.vasaprovider.added | Info | VASA Provider is added to the host. | `VASA Provider {1} is added to the host {2}.` | <ul><li>{1}: Value</li><li>{2}: Host name</li></ul> |
| esx.problem.vvol.vasaprovider.connectivity.lost | Error | Connectivity with the VASA Provider is lost. | `Connectivity with the VASA Provider {1} is lost.` | <ul><li>{1}: Value</li></ul> |
| esx.problem.vvol.vasaprovider.removed | Info | VASA Provider is removed from the host. | `VASA Provider {1} is removed from the host {2}.` | <ul><li>{1}: Value</li><li>{2}: Host name</li></ul> |
| esx.problem.wbem.deprecated | Warning | CIM service on ESXi is deprecated and will be removed in the next major release. CIM is enabled on the host. Please refer to KB 95798 for more details. | `CIM service on ESXi is deprecated and will be removed in the next major release. CIM is enabled on the host. Please refer to KB 95798 for more details.` |  |
| esx.problem.wbem.deprecated.thirdPartyProv | Warning | CIM service on ESXi is deprecated and will be removed in the next major release. There are 3rd party CIM providers ({1}) installed on the host. Please reach out to the 3rd party CIM vendor for a replacement solution. Please refer to KB 95798 for more details. | `CIM service on ESXi is deprecated and will be removed in the next major release. There are 3rd party CIM providers ({1}) installed on the host. Please reach out to the 3rd party CIM vendor for a replacement solution. Please refer to KB 95798 for more details.` | <ul><li>{1}: Value</li></ul> |
| hbr.policy.failedPolicyProcessing | Error | Protection policy deserialization failure. | `Protection policy deserialization failure for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.AppQuiescedDeltaCompletedEvent | Info | Application consistent sync completed. | `Application consistent sync completed for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name} ({bytes} bytes transferred)` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{bytes}: bytes</li></ul> |
| hbr.primary.ConnectionRestoredToHbrServerEvent | Info | Connection to VR Server restored. | `Connection to VR Server restored for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.DeltaAbortedEvent | Warning | Sync stopped. | `Sync stopped for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}: {reason.@enum.hbr.primary.ReasonForDeltaAbort}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{reason}: Reason</li></ul> |
| hbr.primary.DeltaCompletedEvent | Info | Sync completed. | `Sync completed for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name} ({bytes} bytes transferred).` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{bytes}: bytes</li></ul> |
| hbr.primary.DeltaStartedEvent | Info | Sync started. | `Sync started by {userName} for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{userName}: User name</li><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.DemandLogCorrupt | Warning | Demand log header corruption detected. | `Demand log header corruption detected during replication of virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.DisableOfflineError | Warning | Cannot disable the offline instance. | `Could not disable the offline instance during replication of virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}. Stop the offline synchronization and restart it.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.FSQuiescedDeltaCompletedEvent | Info | File system consistent sync completed. | `File system consistent sync completed for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name} ({bytes} bytes transferred)` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{bytes}: bytes</li></ul> |
| hbr.primary.FSQuiescedSnapshot | Warning | Application quiescing failed during replication. | `Application quiescing failed during replication for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}: {reason.@enum.hbr.primary.ReasonForAppQuiesceFailure}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{reason}: Reason</li></ul> |
| hbr.primary.FailedToStartDeltaEvent | Error | Failed to start sync. | `Failed to start sync for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}: {reason.@enum.fault.ReplicationVmFault.ReasonForFault}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{reason}: Reason</li></ul> |
| hbr.primary.FailedToStartSyncEvent | Error | Failed to start full sync. | `Failed to start full sync for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}: {reason.@enum.fault.ReplicationVmFault.ReasonForFault}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{reason}: Reason</li></ul> |
| hbr.primary.HostLicenseFailedEvent | Error | vSphere Replication is not licensed, replication is disabled. | `vSphere Replication is not licensed, replication is disabled on host {host.name} in cluster {computeResource.name} in {datacenter.name}` | <ul><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.HostMaxSupportedNicsConfigurationEvent | Warning | Maximum number of supported replication NICs reached. | `Maximum number of supported replication NICs reached on host {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.InvalidDiskReplicationConfigurationEvent | Error | Disk replication configuration is invalid. | `Replication configuration is invalid for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}, disk {diskKey}: {reasonForFault.@enum.fault.ReplicationDiskConfigFault.ReasonForFault}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{diskKey}: disk Key</li><li>{reasonForFault}: Fault/error</li></ul> |
| hbr.primary.InvalidVmReplicationConfigurationEvent | Error | Virtual machine replication configuration is invalid. | `Replication configuration is invalid for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}: {reasonForFault.@enum.fault.ReplicationVmConfigFault.ReasonForFault}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{reasonForFault}: Fault/error</li></ul> |
| hbr.primary.NetCompressionNotOkForServerEvent | Warning | VR Server does not support network compression. | `VR Server does not support network compression for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.NetCompressionOkForServerEvent | Info | VR Server supports network compression. | `VR Server supports network compression for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.NoConnectionToHbrServerEvent | Warning | No connection to VR Server. | `No connection to VR Server for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}: {reason.@enum.hbr.primary.ReasonForNoServerConnection}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{reason}: Reason</li></ul> |
| hbr.primary.NoProgressWithHbrServerEvent | Error | VR Server error: {reason.@enum.hbr.primary.ReasonForNoServerProgress} | `VR Server error for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}: {reason.@enum.hbr.primary.ReasonForNoServerProgress}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{reason}: Reason</li></ul> |
| hbr.primary.PrepareDeltaTimeExceedsRpoEvent | Warning | Prepare Delta Time exceeds configured RPO. | `Prepare Delta Time exceeds configured RPO for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.PsfFileLost | Error | VR persistent state file was lost. | `VR persistent state file was lost during replication of virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}. System has paused replication.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.QuiesceNotSupported | Warning | Quiescing is not supported for this virtual machine. | `Quiescing is not supported for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.RpoOkForServerEvent | Info | VR Server is compatible with the configured RPO. | `VR Server is compatible with the configured RPO for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.RpoTooLowForServerEvent | Warning | VR Server does not support the configured RPO. | `VR Server does not support the configured RPO for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.SyncCompletedEvent | Info | Full sync completed. | `Full sync completed for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name} ({bytes} bytes transferred).` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{bytes}: bytes</li></ul> |
| hbr.primary.SyncStartedEvent | Info | Full sync started. | `Full sync started by {userName} for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}.` | <ul><li>{userName}: User name</li><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.SystemPausedReplication | Warning | System has paused replication. | `System has paused replication for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}: {reason.@enum.hbr.primary.ReasonForPausedReplication}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{reason}: Reason</li></ul> |
| hbr.primary.UnquiescedDeltaCompletedEvent | Warning | Quiescing failed or the virtual machine is powered off. Unquiesced crash consistent sync completed. | `Quiescing failed or the virtual machine is powered off. Unquiesced crash consistent sync completed for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name} ({bytes} bytes transferred).` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{bytes}: bytes</li></ul> |
| hbr.primary.UnquiescedSnapshot | Warning | Unable to quiesce the guest. | `Unable to quiesce the guest for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}: VSS failure` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.VmLicenseFailedEvent | Error | vSphere Replication is not licensed, replication is disabled. | `vSphere Replication is not licensed, replication is disabled for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name}` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li></ul> |
| hbr.primary.VmReplicationConfigurationChangedEvent | Info | Replication configuration changed. | `Replication configuration changed for virtual machine {vm.name} on host {host.name} in cluster {computeResource.name} in {datacenter.name} ({numDisks} disks, {rpo} minutes RPO, VR Server is {vrServerAddress}:{vrServerPort}).` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{numDisks}: num Disks</li><li>{rpo}: rpo</li><li>{vrServerAddress}: vr Server Address</li><li>{vrServerPort}: vr Server Port</li></ul> |
| vim.event.LicenseDowngradedEvent | Warning | License downgrade | `License downgrade: {licenseKey} removes the following features: {lostFeatures}` | <ul><li>{licenseKey}: license Key</li><li>{lostFeatures}: lost Features</li></ul> |
| vim.event.SubscriptionLicenseExpiredEvent | Warning | The time-limited license on the host has expired. | `The time-limited license on host {host.name} has expired. To comply with the EULA, renew the license at http://my.vmware.com` | <ul><li>{host.name}: Host name</li></ul> |
| vim.event.SystemSwapInaccessible | Warning | System swap inaccessible | `System swap is inaccessible because the datastore '{datastore}' is permanently lost. Please modify the system swap configuration and/or the local system swap directory.` | <ul><li>{datastore}: Datastore</li></ul> |
| vim.event.UnsupportedHardwareVersionEvent | Warning | This virtual machine uses hardware version {version}, which is no longer supported. Upgrade is recommended. | `Virtual machine {vm.name} on {host.name} in cluster {computeResource.name} in {datacenter.name} uses hardware version {version}, which is no longer supported. Upgrade is recommended.` | <ul><li>{vm.name}: VM name</li><li>{host.name}: Host name</li><li>{computeResource.name}: Cluster name</li><li>{datacenter.name}: Datacenter name</li><li>{version}: version</li></ul> |
| vim.event.certificateManager.installCert | Info | Successful installation of a new server certificate. | `User {userName}@{peerIp} has successfully installed a new server certificate.` | <ul><li>{userName}: User name</li><li>{peerIp}: peer Ip</li></ul> |
| vim.event.certificateManager.replaceCaCrl | Info | Successful replacement of the CA and the revocation list. | `User {userName}@{peerIp} has successfully replaced the CA and the revocation list.` | <ul><li>{userName}: User name</li><li>{peerIp}: peer Ip</li></ul> |
| vprob.net.connectivity.lost | Error | Lost Network Connectivity | `Lost network connectivity on virtual switch {1}. Physical NIC {2} is down. Affected portgroups:{3}.` | <ul><li>{1}: Virtual switch</li><li>{2}: Physical NIC</li><li>{3}: Portgroup(s)</li></ul> |
| vprob.net.e1000.tso6.notsupported | Error | No IPv6 TSO support | `Guest-initiated IPv6 TCP Segmentation Offload (TSO) packets ignored. Manually disable TSO inside the guest operating system in virtual machine {1}, or use a different virtual adapter.` | <ul><li>{1}: VM name</li></ul> |
| vprob.net.migrate.bindtovmk | Warning | Invalid vmknic specified in /Migrate/Vmknic | `The ESX advanced config option /Migrate/Vmknic is set to an invalid vmknic: {1}.  /Migrate/Vmknic specifies a vmknic that vMotion binds to for improved performance.  Please update the config option with a valid vmknic or, if you do not want vMotion to bind to a specific vmknic, remove the invalid vmknic and leave the option blank.` | <ul><li>{1}: Value</li></ul> |
| vprob.net.proxyswitch.port.unavailable | Warning | Virtual NIC connection to switch failed | `Virtual NIC with hardware address {1} failed to connect to distributed virtual port {2} on switch {3}. There are no more ports available on the host proxy switch.` | <ul><li>{1}: MAC address</li><li>{2}: Port</li><li>{3}: Virtual switch</li></ul> |
| vprob.net.redundancy.degraded | Warning | Network Redundancy Degraded | `Uplink redundancy degraded on virtual switch {1}. Physical NIC {2} is down. {3} uplinks still up. Affected portgroups:{4}.` | <ul><li>{1}: Virtual switch</li><li>{2}: Physical NIC</li><li>{3}: Uplink count</li><li>{4}: Portgroup(s)</li></ul> |
| vprob.net.redundancy.lost | Warning | Lost Network Redundancy | `Lost uplink redundancy on virtual switch {1}. Physical NIC {2} is down. Affected portgroups:{3}.` | <ul><li>{1}: Virtual switch</li><li>{2}: Physical NIC</li><li>{3}: Portgroup(s)</li></ul> |
| vprob.scsi.device.thinprov.atquota | Warning | Thin Provisioned Device Nearing Capacity | `Space utilization on thin-provisioned device {1} exceeded configured threshold.` | <ul><li>{1}: Device ID</li></ul> |
| vprob.storage.connectivity.lost | Error | Lost Storage Connectivity | `Lost connectivity to storage device {1}. Path {2} is down. Affected datastores: {3}.` | <ul><li>{1}: Device ID</li><li>{2}: Path</li><li>{3}: Datastore(s)</li></ul> |
| vprob.storage.redundancy.degraded | Warning | Degraded Storage Path Redundancy | `Path redundancy to storage device {1} degraded. Path {2} is down. {3} remaining active paths. Affected datastores: {4}.` | <ul><li>{1}: Device ID</li><li>{2}: Path</li><li>{3}: Path count</li><li>{4}: Datastore(s)</li></ul> |
| vprob.storage.redundancy.lost | Warning | Lost Storage Path Redundancy | `Lost path redundancy to storage device {1}. Path {2} is down. Affected datastores: {3}.` | <ul><li>{1}: Device ID</li><li>{2}: Path</li><li>{3}: Datastore(s)</li></ul> |
| vprob.vmfs.error.volume.is.locked | Error | VMFS Locked By Remote Host | `Volume on device {1} is locked, possibly because some remote host encountered an error during a volume operation and could not recover.` | <ul><li>{1}: Device ID</li></ul> |
| vprob.vmfs.extent.offline | Error | Device backing an extent of a file system is offline. | `An attached device {1} might be offline. The file system {2} is now in a degraded state. While the datastore is still available, parts of data that reside on the extent that went offline might be inaccessible.` | <ul><li>{1}: Device ID</li><li>{2}: File system</li></ul> |
| vprob.vmfs.extent.online | Info | Device backing an extent of a file system is online. | `Device {1} backing file system {2} came online. This extent was previously offline. All resources on this device are now available.` | <ul><li>{1}: Device ID</li><li>{2}: File system</li></ul> |
| vprob.vmfs.heartbeat.corruptondisk | Error | VMFS Heartbeat Corruption Detected | `At least one corrupt HB slot was detected on volume {1} ({2}). Other regions of the volume may be damaged too.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| vprob.vmfs.heartbeat.recovered | Info | VMFS Volume Connectivity Restored | `Successfully restored access to volume {1} ({2}) following connectivity issues.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| vprob.vmfs.heartbeat.timedout | Info | VMFS Volume Connectivity Degraded | `Lost access to volume {1} ({2}) due to connectivity issues. Recovery attempt is in progress and outcome will be reported shortly.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| vprob.vmfs.heartbeat.unrecoverable | Error | VMFS Volume Connectivity Lost | `Lost connectivity to volume {1} ({2}) and subsequent recovery attempts have failed.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| vprob.vmfs.journal.createfailed | Error | No Space To Create VMFS Journal | `No space for journal on volume {1} ({2}). Opening volume in read-only metadata mode with limited write support.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| vprob.vmfs.lock.corruptondisk | Error | VMFS Lock Corruption Detected | `At least one corrupt on-disk lock was detected on volume {1} ({2}). Other regions of the volume may be damaged too.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| vprob.vmfs.nfs.server.disconnect | Error | Lost connection to NFS server | `Lost connection to server {1} mount point {2} mounted as {3} ({4}).` | <ul><li>{1}: Server</li><li>{2}: Mount point</li><li>{3}: Mount name</li><li>{4}: Value</li></ul> |
| vprob.vmfs.nfs.server.restored | Info | Restored connection to NFS server | `Restored connection to server {1} mount point {2} mounted as {3} ({4}).` | <ul><li>{1}: Server</li><li>{2}: Mount point</li><li>{3}: Mount name</li><li>{4}: Value</li></ul> |
| vprob.vmfs.resource.corruptondisk | Error | VMFS Resource Corruption Detected | `At least one corrupt resource metadata region was detected on volume {1} ({2}). Other regions of the volume might be damaged too.` | <ul><li>{1}: Volume</li><li>{2}: Volume UUID</li></ul> |
| vprob.vob.vsan.pdl.offline | Error | vSAN device has gone offline. | `vSAN device {1} has gone offline.` | <ul><li>{1}: Device ID</li></ul> |